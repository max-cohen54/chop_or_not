{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b88cb9-4c3e-497c-be72-020a1a83cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 07:14:55.020737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 07:14:56.774255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import train_and_eval_functions as taef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2278055a-5489-4a99-843a-ff2d061729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 1\n",
    "MODEL_PATH = '/global/homes/m/mcohen54/chop_or_not_development/trained_models/trial_4'\n",
    "PLOTS_PATH = MODEL_PATH+'/plots'\n",
    "H_DIM_1 = 16\n",
    "H_DIM_2 = 8\n",
    "LATENT_DIM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd620c0-4582-4e75-ae51-9774dcb2b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booting up...\n",
      "Starting to load data...\n",
      "\n",
      "Loading hChToTauNu_13TeV_PU20.h5...\n",
      "Loading hToTauTau_13TeV_PU20.h5...\n",
      "Loading Ato4l_lepFilter_13TeV.h5...\n",
      "Loading background_for_training.h5...\n",
      "Loading leptoquark_LOWMASS_lepFilter_13TeV.h5...\n",
      "Beginning preprocessing...\n",
      "\n",
      "Load and preprocessing complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = taef.load_and_preprocess(data_path='/global/homes/m/mcohen54/chop_or_not_development/data', standard_scaler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ca4de2-b107-44c4-914a-c027caa09fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training procedure... booting up...\n",
      "\n",
      "Initialization attempt 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 07:15:17.139195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38366 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:03:00.0, compute capability: 8.0\n",
      "2025-04-27 07:15:17.142652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38366 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0\n",
      "2025-04-27 07:15:17.144440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38366 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:82:00.0, compute capability: 8.0\n",
      "2025-04-27 07:15:17.146102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38366 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n",
      "2025-04-27 07:15:17.895985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized network with valid losses!\n",
      "\n",
      "Starting training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 07:15:18.623788: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [2000000,57]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in mean!\n",
      "mean stats: Tensor(\"cond/Min:0\", shape=(), dtype=float32) Tensor(\"cond/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in log_var!\n",
      "log_var stats: Tensor(\"cond_1/Min:0\", shape=(), dtype=float32) Tensor(\"cond_1/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in sampled z!\n",
      "z stats: Tensor(\"cond_2/Min:0\", shape=(), dtype=float32) Tensor(\"cond_2/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in mean!\n",
      "mean stats: Tensor(\"cond/Min:0\", shape=(), dtype=float32) Tensor(\"cond/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in log_var!\n",
      "log_var stats: Tensor(\"cond_1/Min:0\", shape=(), dtype=float32) Tensor(\"cond_1/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in sampled z!\n",
      "z stats: Tensor(\"cond_2/Min:0\", shape=(), dtype=float32) Tensor(\"cond_2/Max:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 07:15:20.131280: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f0a513fca40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-27 07:15:20.131306: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-27 07:15:20.131309: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-27 07:15:20.131312: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-27 07:15:20.131314: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-04-27 07:15:20.135343: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-27 07:15:20.162632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8901\n",
      "2025-04-27 07:15:20.271612: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-04-27 07:16:04.707759: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [1000000,57]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Beta: 0.0000, Learning Rate: 0.010000\n",
      "Total Loss: 22021.5273 - Reconstruction: 22021.5273 - KL: 25097681240064.0000\n",
      "Val Total Loss: 113.7111 - Val Reconstruction: 113.7111 - Val KL: 170.2488\n",
      "Epoch 2/100\n",
      "Beta: 0.0800, Learning Rate: 0.010000\n",
      "Total Loss: 108.3807 - Reconstruction: 108.3807 - KL: 165.2482\n",
      "Val Total Loss: 118.1051 - Val Reconstruction: 104.8432 - Val KL: 165.7742\n",
      "Epoch 3/100\n",
      "Beta: 0.1600, Learning Rate: 0.010000\n",
      "Total Loss: 100.9217 - Reconstruction: 100.9217 - KL: 165.7504\n",
      "Val Total Loss: 132.9496 - Val Reconstruction: 106.0499 - Val KL: 168.1233\n",
      "Epoch 4/100\n",
      "Beta: 0.2400, Learning Rate: 0.010000\n",
      "Total Loss: 101.0428 - Reconstruction: 101.0428 - KL: 161.5596\n",
      "Val Total Loss: 147.8050 - Val Reconstruction: 108.5528 - Val KL: 163.5508\n",
      "Epoch 5/100\n",
      "Beta: 0.3200, Learning Rate: 0.010000\n",
      "Total Loss: 102.4710 - Reconstruction: 102.4710 - KL: 159.8783\n",
      "Val Total Loss: 149.5456 - Val Reconstruction: 98.2555 - Val KL: 160.2816\n",
      "Epoch 6/100\n",
      "Beta: 0.4000, Learning Rate: 0.010000\n",
      "Total Loss: 100.1065 - Reconstruction: 100.1065 - KL: 160.1965\n",
      "Val Total Loss: 162.7500 - Val Reconstruction: 96.9063 - Val KL: 164.6094\n",
      "Epoch 7/100\n",
      "Beta: 0.4800, Learning Rate: 0.010000\n",
      "Total Loss: 95.3251 - Reconstruction: 95.3251 - KL: 160.4216\n",
      "Val Total Loss: 166.1846 - Val Reconstruction: 89.0484 - Val KL: 160.7005\n",
      "Epoch 8/100\n",
      "Beta: 0.5600, Learning Rate: 0.010000\n",
      "Total Loss: 92.2284 - Reconstruction: 92.2284 - KL: 159.7204\n",
      "Val Total Loss: 167.0508 - Val Reconstruction: 78.8689 - Val KL: 157.4676\n",
      "Epoch 9/100\n",
      "Beta: 0.6400, Learning Rate: 0.010000\n",
      "Total Loss: 99.3127 - Reconstruction: 99.3127 - KL: 160.2090\n",
      "Val Total Loss: 213.5629 - Val Reconstruction: 109.8774 - Val KL: 162.0086\n",
      "Epoch 10/100\n",
      "Beta: 0.7200, Learning Rate: 0.010000\n",
      "Total Loss: 99.8901 - Reconstruction: 99.8901 - KL: 161.8854\n",
      "Val Total Loss: 217.1605 - Val Reconstruction: 100.4394 - Val KL: 162.1127\n",
      "Epoch 11/100\n",
      "Beta: 0.8000, Learning Rate: 0.010000\n",
      "Total Loss: 104.0237 - Reconstruction: 104.0237 - KL: 163.5766\n",
      "Val Total Loss: 227.1929 - Val Reconstruction: 95.2228 - Val KL: 164.9626\n",
      "Epoch 12/100\n",
      "Beta: 0.8800, Learning Rate: 0.010000\n",
      "Total Loss: 96.8154 - Reconstruction: 96.8154 - KL: 167.3431\n",
      "Val Total Loss: 244.0396 - Val Reconstruction: 94.6503 - Val KL: 169.7605\n",
      "Epoch 13/100\n",
      "Beta: 0.9600, Learning Rate: 0.010000\n",
      "Total Loss: 100.0001 - Reconstruction: 100.0001 - KL: 169.2323\n",
      "Val Total Loss: 262.7819 - Val Reconstruction: 99.2703 - Val KL: 170.3245\n",
      "Epoch 14/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 105.5136 - Reconstruction: 105.5136 - KL: 169.8463\n",
      "Val Total Loss: 275.8846 - Val Reconstruction: 104.8617 - Val KL: 171.0229\n",
      "Epoch 15/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 102.2886 - Reconstruction: 102.2886 - KL: 170.9150\n",
      "Val Total Loss: 273.2280 - Val Reconstruction: 101.0721 - Val KL: 172.1559\n",
      "Epoch 16/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 104.2826 - Reconstruction: 104.2826 - KL: 168.7167\n",
      "Val Total Loss: 262.0799 - Val Reconstruction: 93.7110 - Val KL: 168.3689\n",
      "Epoch 17/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 116.9133 - Reconstruction: 116.9133 - KL: 3934614784.0000\n",
      "Val Total Loss: 278.4011 - Val Reconstruction: 107.7396 - Val KL: 170.6615\n",
      "Epoch 18/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 100.5857 - Reconstruction: 100.5857 - KL: 170.7091\n",
      "Val Total Loss: 266.4634 - Val Reconstruction: 96.2531 - Val KL: 170.2104\n",
      "Epoch 19/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 103.5378 - Reconstruction: 103.5378 - KL: 170.9184\n",
      "Val Total Loss: 267.0955 - Val Reconstruction: 96.7904 - Val KL: 170.3051\n",
      "Epoch 20/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 103.1777 - Reconstruction: 103.1777 - KL: 170.8939\n",
      "Val Total Loss: 269.3343 - Val Reconstruction: 99.8280 - Val KL: 169.5062\n",
      "Epoch 21/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 100.6046 - Reconstruction: 100.6046 - KL: 170.9833\n",
      "Val Total Loss: 265.9030 - Val Reconstruction: 95.6633 - Val KL: 170.2397\n",
      "Epoch 22/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 100.3092 - Reconstruction: 100.3092 - KL: 169.8713\n",
      "Val Total Loss: 265.9535 - Val Reconstruction: 97.0965 - Val KL: 168.8570\n",
      "Epoch 23/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 99.1799 - Reconstruction: 99.1799 - KL: 169.2598\n",
      "Val Total Loss: 259.8962 - Val Reconstruction: 91.5651 - Val KL: 168.3311\n",
      "Epoch 24/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 99.6166 - Reconstruction: 99.6166 - KL: 168.4326\n",
      "Val Total Loss: 259.4314 - Val Reconstruction: 91.9294 - Val KL: 167.5020\n",
      "Epoch 25/100\n",
      "Beta: 1.0000, Learning Rate: 0.010000\n",
      "Total Loss: 98.2612 - Reconstruction: 98.2612 - KL: 167.9855\n",
      "Val Total Loss: 260.1476 - Val Reconstruction: 91.6851 - Val KL: 168.4625\n",
      "Epoch 26/100\n",
      "Beta: 0.0000, Learning Rate: 0.001000\n",
      "Total Loss: 88.8415 - Reconstruction: 88.8415 - KL: 171.0945\n",
      "Val Total Loss: 89.0882 - Val Reconstruction: 89.0882 - Val KL: 172.4060\n",
      "Epoch 27/100\n",
      "Beta: 0.0800, Learning Rate: 0.001000\n",
      "Total Loss: 89.7702 - Reconstruction: 89.7702 - KL: 172.8668\n",
      "Val Total Loss: 104.2990 - Val Reconstruction: 90.4355 - Val KL: 173.2940\n",
      "Epoch 28/100\n",
      "Beta: 0.1600, Learning Rate: 0.001000\n",
      "Total Loss: 89.2551 - Reconstruction: 89.2551 - KL: 173.5060\n",
      "Val Total Loss: 116.1178 - Val Reconstruction: 88.3728 - Val KL: 173.4062\n",
      "Epoch 29/100\n",
      "Beta: 0.2400, Learning Rate: 0.001000\n",
      "Total Loss: 88.5751 - Reconstruction: 88.5751 - KL: 173.8050\n",
      "Val Total Loss: 131.6685 - Val Reconstruction: 89.9587 - Val KL: 173.7910\n",
      "Epoch 30/100\n",
      "Beta: 0.3200, Learning Rate: 0.001000\n",
      "Total Loss: 87.6585 - Reconstruction: 87.6585 - KL: 174.1333\n",
      "Val Total Loss: 145.1430 - Val Reconstruction: 89.3915 - Val KL: 174.2235\n",
      "Epoch 31/100\n",
      "Beta: 0.4000, Learning Rate: 0.001000\n",
      "Total Loss: 87.9710 - Reconstruction: 87.9710 - KL: 174.3648\n",
      "Val Total Loss: 157.3550 - Val Reconstruction: 87.7868 - Val KL: 173.9205\n",
      "Epoch 32/100\n",
      "Beta: 0.4800, Learning Rate: 0.001000\n",
      "Total Loss: 87.9762 - Reconstruction: 87.9762 - KL: 174.5312\n",
      "Val Total Loss: 171.9205 - Val Reconstruction: 88.1725 - Val KL: 174.4750\n",
      "Epoch 33/100\n",
      "Beta: 0.5600, Learning Rate: 0.001000\n",
      "Total Loss: 86.8902 - Reconstruction: 86.8902 - KL: 174.6215\n",
      "Val Total Loss: 186.6288 - Val Reconstruction: 88.8677 - Val KL: 174.5735\n",
      "Epoch 34/100\n",
      "Beta: 0.6400, Learning Rate: 0.001000\n",
      "Total Loss: 86.5600 - Reconstruction: 86.5600 - KL: 174.7510\n",
      "Val Total Loss: 200.3057 - Val Reconstruction: 88.5659 - Val KL: 174.5936\n",
      "Epoch 35/100\n",
      "Beta: 0.7200, Learning Rate: 0.001000\n",
      "Total Loss: 86.4671 - Reconstruction: 86.4671 - KL: 174.7581\n",
      "Val Total Loss: 212.4680 - Val Reconstruction: 86.7116 - Val KL: 174.6617\n",
      "Epoch 36/100\n",
      "Beta: 0.8000, Learning Rate: 0.001000\n",
      "Total Loss: 86.5285 - Reconstruction: 86.5285 - KL: 174.8450\n",
      "Val Total Loss: 226.1389 - Val Reconstruction: 86.6017 - Val KL: 174.4216\n",
      "Epoch 37/100\n",
      "Beta: 0.8800, Learning Rate: 0.001000\n",
      "Total Loss: 85.7061 - Reconstruction: 85.7061 - KL: 174.8334\n",
      "Val Total Loss: 241.3093 - Val Reconstruction: 87.9089 - Val KL: 174.3187\n",
      "Epoch 38/100\n",
      "Beta: 0.9600, Learning Rate: 0.001000\n",
      "Total Loss: 85.0229 - Reconstruction: 85.0229 - KL: 174.8386\n",
      "Val Total Loss: 251.0340 - Val Reconstruction: 83.4120 - Val KL: 174.6063\n",
      "Epoch 39/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 85.1419 - Reconstruction: 85.1419 - KL: 174.8298\n",
      "Val Total Loss: 259.2880 - Val Reconstruction: 84.3913 - Val KL: 174.8967\n",
      "Epoch 40/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.6870 - Reconstruction: 84.6870 - KL: 174.8762\n",
      "Val Total Loss: 261.4452 - Val Reconstruction: 86.9541 - Val KL: 174.4910\n",
      "Epoch 41/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.8857 - Reconstruction: 84.8857 - KL: 174.8320\n",
      "Val Total Loss: 259.8154 - Val Reconstruction: 84.7617 - Val KL: 175.0538\n",
      "Epoch 42/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.8900 - Reconstruction: 84.8900 - KL: 174.8850\n",
      "Val Total Loss: 260.3073 - Val Reconstruction: 85.1250 - Val KL: 175.1823\n",
      "Epoch 43/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.7535 - Reconstruction: 84.7535 - KL: 174.9201\n",
      "Val Total Loss: 259.9171 - Val Reconstruction: 84.7436 - Val KL: 175.1735\n",
      "Epoch 44/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.5465 - Reconstruction: 84.5465 - KL: 174.9175\n",
      "Val Total Loss: 259.7319 - Val Reconstruction: 84.7080 - Val KL: 175.0239\n",
      "Epoch 45/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.2762 - Reconstruction: 84.2762 - KL: 175.0793\n",
      "Val Total Loss: 258.8821 - Val Reconstruction: 83.7616 - Val KL: 175.1205\n",
      "Epoch 46/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.4608 - Reconstruction: 84.4608 - KL: 175.0390\n",
      "Val Total Loss: 258.4251 - Val Reconstruction: 83.4131 - Val KL: 175.0121\n",
      "Epoch 47/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.6501 - Reconstruction: 84.6501 - KL: 175.0294\n",
      "Val Total Loss: 259.2363 - Val Reconstruction: 84.6037 - Val KL: 174.6326\n",
      "Epoch 48/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.0731 - Reconstruction: 84.0731 - KL: 175.1174\n",
      "Val Total Loss: 259.5758 - Val Reconstruction: 84.3117 - Val KL: 175.2641\n",
      "Epoch 49/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.3124 - Reconstruction: 84.3124 - KL: 175.2000\n",
      "Val Total Loss: 257.9352 - Val Reconstruction: 82.4502 - Val KL: 175.4850\n",
      "Epoch 50/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 83.6573 - Reconstruction: 83.6573 - KL: 175.3119\n",
      "Val Total Loss: 260.6130 - Val Reconstruction: 85.1328 - Val KL: 175.4802\n",
      "Epoch 51/100\n",
      "Beta: 0.0000, Learning Rate: 0.001000\n",
      "Total Loss: 83.9831 - Reconstruction: 83.9831 - KL: 175.2901\n",
      "Val Total Loss: 83.4353 - Val Reconstruction: 83.4353 - Val KL: 175.2934\n",
      "Epoch 52/100\n",
      "Beta: 0.0800, Learning Rate: 0.001000\n",
      "Total Loss: 84.1904 - Reconstruction: 84.1904 - KL: 175.3090\n",
      "Val Total Loss: 98.1282 - Val Reconstruction: 84.1114 - Val KL: 175.2108\n",
      "Epoch 53/100\n",
      "Beta: 0.1600, Learning Rate: 0.001000\n",
      "Total Loss: 84.1994 - Reconstruction: 84.1994 - KL: 175.3361\n",
      "Val Total Loss: 114.5120 - Val Reconstruction: 86.4195 - Val KL: 175.5781\n",
      "Epoch 54/100\n",
      "Beta: 0.2400, Learning Rate: 0.001000\n",
      "Total Loss: 84.4408 - Reconstruction: 84.4408 - KL: 175.3597\n",
      "Val Total Loss: 125.7143 - Val Reconstruction: 83.5881 - Val KL: 175.5260\n",
      "Epoch 55/100\n",
      "Beta: 0.3200, Learning Rate: 0.001000\n",
      "Total Loss: 84.3498 - Reconstruction: 84.3498 - KL: 175.4324\n",
      "Val Total Loss: 139.5158 - Val Reconstruction: 83.3856 - Val KL: 175.4068\n",
      "Epoch 56/100\n",
      "Beta: 0.4000, Learning Rate: 0.001000\n",
      "Total Loss: 84.1225 - Reconstruction: 84.1225 - KL: 175.4607\n",
      "Val Total Loss: 153.8532 - Val Reconstruction: 83.7854 - Val KL: 175.1694\n",
      "Epoch 57/100\n",
      "Beta: 0.4800, Learning Rate: 0.001000\n",
      "Total Loss: 84.2994 - Reconstruction: 84.2994 - KL: 175.3461\n",
      "Val Total Loss: 166.5271 - Val Reconstruction: 82.4137 - Val KL: 175.2362\n",
      "Epoch 58/100\n",
      "Beta: 0.5600, Learning Rate: 0.001000\n",
      "Total Loss: 83.8503 - Reconstruction: 83.8503 - KL: 175.4221\n",
      "Val Total Loss: 181.3467 - Val Reconstruction: 83.1110 - Val KL: 175.4209\n",
      "Epoch 59/100\n",
      "Beta: 0.6400, Learning Rate: 0.001000\n",
      "Total Loss: 84.1409 - Reconstruction: 84.1409 - KL: 175.4490\n",
      "Val Total Loss: 194.9874 - Val Reconstruction: 82.7098 - Val KL: 175.4338\n",
      "Epoch 60/100\n",
      "Beta: 0.7200, Learning Rate: 0.001000\n",
      "Total Loss: 84.7710 - Reconstruction: 84.7710 - KL: 175.3440\n",
      "Val Total Loss: 211.0230 - Val Reconstruction: 84.8522 - Val KL: 175.2372\n",
      "Epoch 61/100\n",
      "Beta: 0.8000, Learning Rate: 0.001000\n",
      "Total Loss: 84.4525 - Reconstruction: 84.4525 - KL: 175.2836\n",
      "Val Total Loss: 223.4493 - Val Reconstruction: 83.1053 - Val KL: 175.4300\n",
      "Epoch 62/100\n",
      "Beta: 0.8800, Learning Rate: 0.001000\n",
      "Total Loss: 83.7506 - Reconstruction: 83.7506 - KL: 175.3265\n",
      "Val Total Loss: 238.5304 - Val Reconstruction: 84.0294 - Val KL: 175.5694\n",
      "Epoch 63/100\n",
      "Beta: 0.9600, Learning Rate: 0.001000\n",
      "Total Loss: 83.9191 - Reconstruction: 83.9191 - KL: 175.3570\n",
      "Val Total Loss: 250.5251 - Val Reconstruction: 82.3987 - Val KL: 175.1316\n",
      "Epoch 64/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.2644 - Reconstruction: 84.2644 - KL: 175.3270\n",
      "Val Total Loss: 260.0982 - Val Reconstruction: 84.5259 - Val KL: 175.5723\n",
      "Epoch 65/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 83.9756 - Reconstruction: 83.9756 - KL: 175.2546\n",
      "Val Total Loss: 259.1489 - Val Reconstruction: 83.8553 - Val KL: 175.2936\n",
      "Epoch 66/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 83.9716 - Reconstruction: 83.9716 - KL: 175.2887\n",
      "Val Total Loss: 261.0416 - Val Reconstruction: 85.7988 - Val KL: 175.2427\n",
      "Epoch 67/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 83.8761 - Reconstruction: 83.8761 - KL: 175.2022\n",
      "Val Total Loss: 259.2359 - Val Reconstruction: 84.1669 - Val KL: 175.0690\n",
      "Epoch 68/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 83.9883 - Reconstruction: 83.9883 - KL: 175.2580\n",
      "Val Total Loss: 261.2721 - Val Reconstruction: 85.6551 - Val KL: 175.6170\n",
      "Epoch 69/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.5736 - Reconstruction: 84.5736 - KL: 175.2430\n",
      "Val Total Loss: 258.4604 - Val Reconstruction: 83.1646 - Val KL: 175.2959\n",
      "Epoch 70/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.3410 - Reconstruction: 84.3410 - KL: 175.3088\n",
      "Val Total Loss: 259.3680 - Val Reconstruction: 83.9944 - Val KL: 175.3736\n",
      "Epoch 71/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.2958 - Reconstruction: 84.2958 - KL: 175.4007\n",
      "Val Total Loss: 260.0598 - Val Reconstruction: 84.5024 - Val KL: 175.5574\n",
      "Epoch 72/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.7165 - Reconstruction: 84.7165 - KL: 175.1217\n",
      "Val Total Loss: 258.7401 - Val Reconstruction: 83.7567 - Val KL: 174.9833\n",
      "Epoch 73/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 85.2504 - Reconstruction: 85.2504 - KL: 175.1595\n",
      "Val Total Loss: 259.9161 - Val Reconstruction: 84.7075 - Val KL: 175.2086\n",
      "Epoch 74/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 85.0953 - Reconstruction: 85.0953 - KL: 175.1780\n",
      "Val Total Loss: 259.3604 - Val Reconstruction: 83.8608 - Val KL: 175.4996\n",
      "Epoch 75/100\n",
      "Beta: 1.0000, Learning Rate: 0.001000\n",
      "Total Loss: 84.4858 - Reconstruction: 84.4858 - KL: 175.1507\n",
      "Val Total Loss: 258.0598 - Val Reconstruction: 83.1563 - Val KL: 174.9035\n",
      "Epoch 76/100\n",
      "Beta: 0.0000, Learning Rate: 0.000100\n",
      "Total Loss: 83.5635 - Reconstruction: 83.5635 - KL: 174.7492\n",
      "Val Total Loss: 82.5291 - Val Reconstruction: 82.5291 - Val KL: 174.6749\n",
      "Epoch 77/100\n",
      "Beta: 0.0800, Learning Rate: 0.000100\n",
      "Total Loss: 83.9639 - Reconstruction: 83.9639 - KL: 174.4713\n",
      "Val Total Loss: 96.7475 - Val Reconstruction: 82.7922 - Val KL: 174.4416\n",
      "Epoch 78/100\n",
      "Beta: 0.1600, Learning Rate: 0.000100\n",
      "Total Loss: 84.0080 - Reconstruction: 84.0080 - KL: 174.3105\n",
      "Val Total Loss: 110.7929 - Val Reconstruction: 82.9053 - Val KL: 174.2973\n",
      "Epoch 79/100\n",
      "Beta: 0.2400, Learning Rate: 0.000100\n",
      "Total Loss: 84.0169 - Reconstruction: 84.0169 - KL: 174.2003\n",
      "Val Total Loss: 124.9438 - Val Reconstruction: 83.1333 - Val KL: 174.2103\n",
      "Epoch 80/100\n",
      "Beta: 0.3200, Learning Rate: 0.000100\n",
      "Total Loss: 84.1505 - Reconstruction: 84.1505 - KL: 174.0528\n",
      "Val Total Loss: 138.9745 - Val Reconstruction: 83.2531 - Val KL: 174.1293\n",
      "Epoch 81/100\n",
      "Beta: 0.4000, Learning Rate: 0.000100\n",
      "Total Loss: 84.3905 - Reconstruction: 84.3905 - KL: 173.8966\n",
      "Val Total Loss: 153.0237 - Val Reconstruction: 83.5194 - Val KL: 173.7607\n",
      "Epoch 82/100\n",
      "Beta: 0.4800, Learning Rate: 0.000100\n",
      "Total Loss: 84.4844 - Reconstruction: 84.4844 - KL: 173.7214\n",
      "Val Total Loss: 167.0255 - Val Reconstruction: 83.5941 - Val KL: 173.8153\n",
      "Epoch 83/100\n",
      "Beta: 0.5600, Learning Rate: 0.000100\n",
      "Total Loss: 84.4240 - Reconstruction: 84.4240 - KL: 173.7315\n",
      "Val Total Loss: 180.9576 - Val Reconstruction: 83.6144 - Val KL: 173.8273\n",
      "Epoch 84/100\n",
      "Beta: 0.6400, Learning Rate: 0.000100\n",
      "Total Loss: 84.4876 - Reconstruction: 84.4876 - KL: 173.6892\n",
      "Val Total Loss: 194.7986 - Val Reconstruction: 83.5675 - Val KL: 173.7986\n",
      "Epoch 85/100\n",
      "Beta: 0.7200, Learning Rate: 0.000100\n",
      "Total Loss: 84.5171 - Reconstruction: 84.5171 - KL: 173.6488\n",
      "Val Total Loss: 208.5280 - Val Reconstruction: 83.4475 - Val KL: 173.7229\n",
      "Epoch 86/100\n",
      "Beta: 0.8000, Learning Rate: 0.000100\n",
      "Total Loss: 84.4388 - Reconstruction: 84.4388 - KL: 173.6736\n",
      "Val Total Loss: 222.4139 - Val Reconstruction: 83.4654 - Val KL: 173.6856\n",
      "Epoch 87/100\n",
      "Beta: 0.8800, Learning Rate: 0.000100\n",
      "Total Loss: 84.4649 - Reconstruction: 84.4649 - KL: 173.6699\n",
      "Val Total Loss: 236.1842 - Val Reconstruction: 83.3685 - Val KL: 173.6543\n",
      "Epoch 88/100\n",
      "Beta: 0.9600, Learning Rate: 0.000100\n",
      "Total Loss: 84.4723 - Reconstruction: 84.4723 - KL: 173.6695\n",
      "Val Total Loss: 250.1827 - Val Reconstruction: 83.4930 - Val KL: 173.6351\n",
      "Epoch 89/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.5196 - Reconstruction: 84.5196 - KL: 173.7316\n",
      "Val Total Loss: 257.0113 - Val Reconstruction: 83.3407 - Val KL: 173.6705\n",
      "Epoch 90/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.3849 - Reconstruction: 84.3849 - KL: 173.7713\n",
      "Val Total Loss: 257.0073 - Val Reconstruction: 83.2807 - Val KL: 173.7266\n",
      "Epoch 91/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.2975 - Reconstruction: 84.2975 - KL: 173.8132\n",
      "Val Total Loss: 257.3312 - Val Reconstruction: 83.4163 - Val KL: 173.9149\n",
      "Epoch 92/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.2092 - Reconstruction: 84.2092 - KL: 173.8269\n",
      "Val Total Loss: 256.9297 - Val Reconstruction: 83.1282 - Val KL: 173.8015\n",
      "Epoch 93/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.1776 - Reconstruction: 84.1776 - KL: 173.8924\n",
      "Val Total Loss: 257.0120 - Val Reconstruction: 83.2331 - Val KL: 173.7789\n",
      "Epoch 94/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.2507 - Reconstruction: 84.2507 - KL: 173.9437\n",
      "Val Total Loss: 257.6914 - Val Reconstruction: 83.5855 - Val KL: 174.1059\n",
      "Epoch 95/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.1918 - Reconstruction: 84.1918 - KL: 173.9492\n",
      "Val Total Loss: 257.2079 - Val Reconstruction: 83.2102 - Val KL: 173.9977\n",
      "Epoch 96/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.2262 - Reconstruction: 84.2262 - KL: 174.0122\n",
      "Val Total Loss: 257.0246 - Val Reconstruction: 83.0670 - Val KL: 173.9575\n",
      "Epoch 97/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.2181 - Reconstruction: 84.2181 - KL: 174.0623\n",
      "Val Total Loss: 257.2213 - Val Reconstruction: 83.1067 - Val KL: 174.1146\n",
      "Epoch 98/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.1761 - Reconstruction: 84.1761 - KL: 174.1219\n",
      "Val Total Loss: 257.1561 - Val Reconstruction: 83.1181 - Val KL: 174.0379\n",
      "Epoch 99/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.1939 - Reconstruction: 84.1939 - KL: 174.0952\n",
      "Val Total Loss: 257.2836 - Val Reconstruction: 83.2254 - Val KL: 174.0582\n",
      "Epoch 100/100\n",
      "Beta: 1.0000, Learning Rate: 0.000100\n",
      "Total Loss: 84.0648 - Reconstruction: 84.0648 - KL: 174.1494\n",
      "Val Total Loss: 257.0204 - Val Reconstruction: 82.8596 - Val KL: 174.1608\n",
      "Training complete!\n",
      "Saving model...\n",
      "Training history plots saved to /global/homes/m/mcohen54/chop_or_not_development/trained_models/trial_4/training_history.png\n",
      "Model saved! Powering down...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_loss': [22021.52734375,\n",
       "  108.38066864013672,\n",
       "  100.9217300415039,\n",
       "  101.04283142089844,\n",
       "  102.47100830078125,\n",
       "  100.10648345947266,\n",
       "  95.32508087158203,\n",
       "  92.22843170166016,\n",
       "  99.31267547607422,\n",
       "  99.89009857177734,\n",
       "  104.02374267578125,\n",
       "  96.81538391113281,\n",
       "  100.00007629394531,\n",
       "  105.51362609863281,\n",
       "  102.28860473632812,\n",
       "  104.2825698852539,\n",
       "  116.9133071899414,\n",
       "  100.58570861816406,\n",
       "  103.53781127929688,\n",
       "  103.17765045166016,\n",
       "  100.60458374023438,\n",
       "  100.30919647216797,\n",
       "  99.17985534667969,\n",
       "  99.61656188964844,\n",
       "  98.26119995117188,\n",
       "  88.84149932861328,\n",
       "  89.77023315429688,\n",
       "  89.2551040649414,\n",
       "  88.57510375976562,\n",
       "  87.65850067138672,\n",
       "  87.97095489501953,\n",
       "  87.97622680664062,\n",
       "  86.89017486572266,\n",
       "  86.55996704101562,\n",
       "  86.46708679199219,\n",
       "  86.52845001220703,\n",
       "  85.7060546875,\n",
       "  85.0228500366211,\n",
       "  85.14192199707031,\n",
       "  84.68695068359375,\n",
       "  84.88570404052734,\n",
       "  84.89004516601562,\n",
       "  84.75348663330078,\n",
       "  84.54654693603516,\n",
       "  84.27621459960938,\n",
       "  84.4608383178711,\n",
       "  84.65010833740234,\n",
       "  84.07308959960938,\n",
       "  84.31243133544922,\n",
       "  83.6573257446289,\n",
       "  83.9831314086914,\n",
       "  84.19041442871094,\n",
       "  84.19937896728516,\n",
       "  84.44075012207031,\n",
       "  84.3498306274414,\n",
       "  84.12247467041016,\n",
       "  84.29944610595703,\n",
       "  83.85034942626953,\n",
       "  84.14085388183594,\n",
       "  84.77098846435547,\n",
       "  84.45247650146484,\n",
       "  83.75061798095703,\n",
       "  83.91912078857422,\n",
       "  84.26439666748047,\n",
       "  83.97557067871094,\n",
       "  83.97161865234375,\n",
       "  83.87611389160156,\n",
       "  83.98827362060547,\n",
       "  84.5735855102539,\n",
       "  84.3410415649414,\n",
       "  84.29576873779297,\n",
       "  84.71647644042969,\n",
       "  85.25042724609375,\n",
       "  85.0953140258789,\n",
       "  84.48577117919922,\n",
       "  83.56350708007812,\n",
       "  83.96389770507812,\n",
       "  84.00801849365234,\n",
       "  84.01691436767578,\n",
       "  84.15049743652344,\n",
       "  84.39054107666016,\n",
       "  84.48436737060547,\n",
       "  84.42400360107422,\n",
       "  84.48762512207031,\n",
       "  84.51707458496094,\n",
       "  84.43877410888672,\n",
       "  84.46489715576172,\n",
       "  84.4723129272461,\n",
       "  84.51961517333984,\n",
       "  84.3849105834961,\n",
       "  84.29752349853516,\n",
       "  84.209228515625,\n",
       "  84.17755126953125,\n",
       "  84.25071716308594,\n",
       "  84.19181060791016,\n",
       "  84.22616577148438,\n",
       "  84.21807861328125,\n",
       "  84.1761474609375,\n",
       "  84.19388580322266,\n",
       "  84.06475067138672],\n",
       " 'reconstruction_loss': [22021.52734375,\n",
       "  108.38066864013672,\n",
       "  100.9217300415039,\n",
       "  101.04283142089844,\n",
       "  102.47100830078125,\n",
       "  100.10648345947266,\n",
       "  95.32508087158203,\n",
       "  92.22843170166016,\n",
       "  99.31267547607422,\n",
       "  99.89009857177734,\n",
       "  104.02374267578125,\n",
       "  96.81538391113281,\n",
       "  100.00007629394531,\n",
       "  105.51362609863281,\n",
       "  102.28860473632812,\n",
       "  104.2825698852539,\n",
       "  116.9133071899414,\n",
       "  100.58570861816406,\n",
       "  103.53781127929688,\n",
       "  103.17765045166016,\n",
       "  100.60458374023438,\n",
       "  100.30919647216797,\n",
       "  99.17985534667969,\n",
       "  99.61656188964844,\n",
       "  98.26119995117188,\n",
       "  88.84149932861328,\n",
       "  89.77023315429688,\n",
       "  89.2551040649414,\n",
       "  88.57510375976562,\n",
       "  87.65850067138672,\n",
       "  87.97095489501953,\n",
       "  87.97622680664062,\n",
       "  86.89017486572266,\n",
       "  86.55996704101562,\n",
       "  86.46708679199219,\n",
       "  86.52845001220703,\n",
       "  85.7060546875,\n",
       "  85.0228500366211,\n",
       "  85.14192199707031,\n",
       "  84.68695068359375,\n",
       "  84.88570404052734,\n",
       "  84.89004516601562,\n",
       "  84.75348663330078,\n",
       "  84.54654693603516,\n",
       "  84.27621459960938,\n",
       "  84.4608383178711,\n",
       "  84.65010833740234,\n",
       "  84.07308959960938,\n",
       "  84.31243133544922,\n",
       "  83.6573257446289,\n",
       "  83.9831314086914,\n",
       "  84.19041442871094,\n",
       "  84.19937896728516,\n",
       "  84.44075012207031,\n",
       "  84.3498306274414,\n",
       "  84.12247467041016,\n",
       "  84.29944610595703,\n",
       "  83.85034942626953,\n",
       "  84.14085388183594,\n",
       "  84.77098846435547,\n",
       "  84.45247650146484,\n",
       "  83.75061798095703,\n",
       "  83.91912078857422,\n",
       "  84.26439666748047,\n",
       "  83.97557067871094,\n",
       "  83.97161865234375,\n",
       "  83.87611389160156,\n",
       "  83.98827362060547,\n",
       "  84.5735855102539,\n",
       "  84.3410415649414,\n",
       "  84.29576873779297,\n",
       "  84.71647644042969,\n",
       "  85.25042724609375,\n",
       "  85.0953140258789,\n",
       "  84.48577117919922,\n",
       "  83.56350708007812,\n",
       "  83.96389770507812,\n",
       "  84.00801849365234,\n",
       "  84.01691436767578,\n",
       "  84.15049743652344,\n",
       "  84.39054107666016,\n",
       "  84.48436737060547,\n",
       "  84.42400360107422,\n",
       "  84.48762512207031,\n",
       "  84.51707458496094,\n",
       "  84.43877410888672,\n",
       "  84.46489715576172,\n",
       "  84.4723129272461,\n",
       "  84.51961517333984,\n",
       "  84.3849105834961,\n",
       "  84.29752349853516,\n",
       "  84.209228515625,\n",
       "  84.17755126953125,\n",
       "  84.25071716308594,\n",
       "  84.19181060791016,\n",
       "  84.22616577148438,\n",
       "  84.21807861328125,\n",
       "  84.1761474609375,\n",
       "  84.19388580322266,\n",
       "  84.06475067138672],\n",
       " 'kl_loss': [25097681240064.0,\n",
       "  165.24819946289062,\n",
       "  165.7503662109375,\n",
       "  161.5596160888672,\n",
       "  159.87826538085938,\n",
       "  160.1964874267578,\n",
       "  160.42156982421875,\n",
       "  159.7204132080078,\n",
       "  160.20901489257812,\n",
       "  161.88540649414062,\n",
       "  163.5766143798828,\n",
       "  167.34307861328125,\n",
       "  169.2322998046875,\n",
       "  169.84632873535156,\n",
       "  170.9149932861328,\n",
       "  168.71669006347656,\n",
       "  3934614784.0,\n",
       "  170.70912170410156,\n",
       "  170.91839599609375,\n",
       "  170.89393615722656,\n",
       "  170.98329162597656,\n",
       "  169.871337890625,\n",
       "  169.25975036621094,\n",
       "  168.43260192871094,\n",
       "  167.98545837402344,\n",
       "  171.0945281982422,\n",
       "  172.86683654785156,\n",
       "  173.50604248046875,\n",
       "  173.80502319335938,\n",
       "  174.1332550048828,\n",
       "  174.36480712890625,\n",
       "  174.5311737060547,\n",
       "  174.6215362548828,\n",
       "  174.75096130371094,\n",
       "  174.7581329345703,\n",
       "  174.8450164794922,\n",
       "  174.8333740234375,\n",
       "  174.83856201171875,\n",
       "  174.82981872558594,\n",
       "  174.87619018554688,\n",
       "  174.83197021484375,\n",
       "  174.88499450683594,\n",
       "  174.92013549804688,\n",
       "  174.91754150390625,\n",
       "  175.079345703125,\n",
       "  175.03904724121094,\n",
       "  175.02935791015625,\n",
       "  175.11737060546875,\n",
       "  175.19996643066406,\n",
       "  175.31187438964844,\n",
       "  175.29014587402344,\n",
       "  175.30902099609375,\n",
       "  175.33612060546875,\n",
       "  175.3596954345703,\n",
       "  175.4324493408203,\n",
       "  175.4607391357422,\n",
       "  175.34605407714844,\n",
       "  175.42214965820312,\n",
       "  175.44895935058594,\n",
       "  175.34396362304688,\n",
       "  175.2836151123047,\n",
       "  175.32652282714844,\n",
       "  175.3570098876953,\n",
       "  175.32701110839844,\n",
       "  175.2545623779297,\n",
       "  175.28866577148438,\n",
       "  175.20217895507812,\n",
       "  175.2579803466797,\n",
       "  175.24301147460938,\n",
       "  175.30880737304688,\n",
       "  175.40074157714844,\n",
       "  175.12168884277344,\n",
       "  175.15948486328125,\n",
       "  175.17803955078125,\n",
       "  175.1506805419922,\n",
       "  174.7491912841797,\n",
       "  174.4713134765625,\n",
       "  174.31048583984375,\n",
       "  174.2002716064453,\n",
       "  174.05279541015625,\n",
       "  173.8966064453125,\n",
       "  173.7213897705078,\n",
       "  173.7314910888672,\n",
       "  173.689208984375,\n",
       "  173.6488494873047,\n",
       "  173.67359924316406,\n",
       "  173.66990661621094,\n",
       "  173.66954040527344,\n",
       "  173.73155212402344,\n",
       "  173.77125549316406,\n",
       "  173.8131561279297,\n",
       "  173.82687377929688,\n",
       "  173.89244079589844,\n",
       "  173.94374084472656,\n",
       "  173.94918823242188,\n",
       "  174.01222229003906,\n",
       "  174.0623321533203,\n",
       "  174.12188720703125,\n",
       "  174.09523010253906,\n",
       "  174.14944458007812],\n",
       " 'val_total_loss': [113.71106719970703,\n",
       "  118.1051025390625,\n",
       "  132.9495849609375,\n",
       "  147.80496215820312,\n",
       "  149.5455780029297,\n",
       "  162.7500457763672,\n",
       "  166.18460083007812,\n",
       "  167.05078125,\n",
       "  213.56288146972656,\n",
       "  217.16053771972656,\n",
       "  227.1929168701172,\n",
       "  244.03955078125,\n",
       "  262.7818603515625,\n",
       "  275.8845520019531,\n",
       "  273.22802734375,\n",
       "  262.0799255371094,\n",
       "  278.40106201171875,\n",
       "  266.46343994140625,\n",
       "  267.0954895019531,\n",
       "  269.3342590332031,\n",
       "  265.90301513671875,\n",
       "  265.9534912109375,\n",
       "  259.896240234375,\n",
       "  259.4313659667969,\n",
       "  260.1475524902344,\n",
       "  89.08821868896484,\n",
       "  104.29901885986328,\n",
       "  116.1177978515625,\n",
       "  131.6685028076172,\n",
       "  145.14300537109375,\n",
       "  157.35499572753906,\n",
       "  171.9205322265625,\n",
       "  186.6288299560547,\n",
       "  200.3057403564453,\n",
       "  212.46800231933594,\n",
       "  226.13894653320312,\n",
       "  241.309326171875,\n",
       "  251.0340118408203,\n",
       "  259.2879638671875,\n",
       "  261.4451599121094,\n",
       "  259.8154296875,\n",
       "  260.3072509765625,\n",
       "  259.9171142578125,\n",
       "  259.7319030761719,\n",
       "  258.882080078125,\n",
       "  258.42510986328125,\n",
       "  259.2362976074219,\n",
       "  259.5758361816406,\n",
       "  257.9352111816406,\n",
       "  260.6130065917969,\n",
       "  83.43534851074219,\n",
       "  98.12824249267578,\n",
       "  114.5119857788086,\n",
       "  125.71432495117188,\n",
       "  139.51580810546875,\n",
       "  153.85316467285156,\n",
       "  166.52713012695312,\n",
       "  181.34666442871094,\n",
       "  194.9874267578125,\n",
       "  211.02297973632812,\n",
       "  223.4492645263672,\n",
       "  238.5303955078125,\n",
       "  250.52511596679688,\n",
       "  260.0981750488281,\n",
       "  259.14892578125,\n",
       "  261.04156494140625,\n",
       "  259.23590087890625,\n",
       "  261.2720642089844,\n",
       "  258.4604187011719,\n",
       "  259.3679504394531,\n",
       "  260.0597839355469,\n",
       "  258.74005126953125,\n",
       "  259.91607666015625,\n",
       "  259.3603820800781,\n",
       "  258.059814453125,\n",
       "  82.52908325195312,\n",
       "  96.74748992919922,\n",
       "  110.79286193847656,\n",
       "  124.94379425048828,\n",
       "  138.97450256347656,\n",
       "  153.02371215820312,\n",
       "  167.0254669189453,\n",
       "  180.9576416015625,\n",
       "  194.79859924316406,\n",
       "  208.5279998779297,\n",
       "  222.4138946533203,\n",
       "  236.1842498779297,\n",
       "  250.18267822265625,\n",
       "  257.01129150390625,\n",
       "  257.00726318359375,\n",
       "  257.3312072753906,\n",
       "  256.9296875,\n",
       "  257.01202392578125,\n",
       "  257.69140625,\n",
       "  257.2078857421875,\n",
       "  257.0245666503906,\n",
       "  257.2213439941406,\n",
       "  257.15606689453125,\n",
       "  257.28363037109375,\n",
       "  257.0204162597656],\n",
       " 'val_reconstruction_loss': [113.71106719970703,\n",
       "  104.84317779541016,\n",
       "  106.04985046386719,\n",
       "  108.55276489257812,\n",
       "  98.2554702758789,\n",
       "  96.90628051757812,\n",
       "  89.04835510253906,\n",
       "  78.86893463134766,\n",
       "  109.87738037109375,\n",
       "  100.43936920166016,\n",
       "  95.22281646728516,\n",
       "  94.65028381347656,\n",
       "  99.27030944824219,\n",
       "  104.86165618896484,\n",
       "  101.07211303710938,\n",
       "  93.71099090576172,\n",
       "  107.73957824707031,\n",
       "  96.2530746459961,\n",
       "  96.79039764404297,\n",
       "  99.82801818847656,\n",
       "  95.66328430175781,\n",
       "  97.09648895263672,\n",
       "  91.5650863647461,\n",
       "  91.929443359375,\n",
       "  91.68507385253906,\n",
       "  89.08821868896484,\n",
       "  90.43549346923828,\n",
       "  88.37281036376953,\n",
       "  89.95867156982422,\n",
       "  89.3914794921875,\n",
       "  87.78681945800781,\n",
       "  88.17250061035156,\n",
       "  88.86769104003906,\n",
       "  88.56586456298828,\n",
       "  86.71157836914062,\n",
       "  86.60165405273438,\n",
       "  87.90890502929688,\n",
       "  83.4120101928711,\n",
       "  84.39127349853516,\n",
       "  86.95411682128906,\n",
       "  84.76166534423828,\n",
       "  85.12499237060547,\n",
       "  84.74364471435547,\n",
       "  84.70799255371094,\n",
       "  83.76155090332031,\n",
       "  83.41305541992188,\n",
       "  84.60374450683594,\n",
       "  84.31170654296875,\n",
       "  82.45022583007812,\n",
       "  85.1328125,\n",
       "  83.43534851074219,\n",
       "  84.11137390136719,\n",
       "  86.41949462890625,\n",
       "  83.58808898925781,\n",
       "  83.3856201171875,\n",
       "  83.78540802001953,\n",
       "  82.41373443603516,\n",
       "  83.1109619140625,\n",
       "  82.70980072021484,\n",
       "  84.8521728515625,\n",
       "  83.10530853271484,\n",
       "  84.02935791015625,\n",
       "  82.39874267578125,\n",
       "  84.52590942382812,\n",
       "  83.85533905029297,\n",
       "  85.79884338378906,\n",
       "  84.1668701171875,\n",
       "  85.65513610839844,\n",
       "  83.16455078125,\n",
       "  83.99435424804688,\n",
       "  84.50240325927734,\n",
       "  83.7567367553711,\n",
       "  84.70745086669922,\n",
       "  83.86077880859375,\n",
       "  83.15634155273438,\n",
       "  82.52908325195312,\n",
       "  82.79216003417969,\n",
       "  82.90528869628906,\n",
       "  83.13331604003906,\n",
       "  83.25310516357422,\n",
       "  83.51943969726562,\n",
       "  83.5941390991211,\n",
       "  83.6143798828125,\n",
       "  83.56751251220703,\n",
       "  83.44747161865234,\n",
       "  83.46544647216797,\n",
       "  83.36852264404297,\n",
       "  83.49301147460938,\n",
       "  83.34074401855469,\n",
       "  83.28065490722656,\n",
       "  83.41632080078125,\n",
       "  83.128173828125,\n",
       "  83.23311614990234,\n",
       "  83.58546447753906,\n",
       "  83.2102279663086,\n",
       "  83.0670394897461,\n",
       "  83.10672760009766,\n",
       "  83.11810302734375,\n",
       "  83.2254409790039,\n",
       "  82.8595962524414],\n",
       " 'val_kl_loss': [170.248779296875,\n",
       "  165.774169921875,\n",
       "  168.12327575683594,\n",
       "  163.55076599121094,\n",
       "  160.2816162109375,\n",
       "  164.6094207763672,\n",
       "  160.70050048828125,\n",
       "  157.46759033203125,\n",
       "  162.0085906982422,\n",
       "  162.11273193359375,\n",
       "  164.9626007080078,\n",
       "  169.76052856445312,\n",
       "  170.32452392578125,\n",
       "  171.02288818359375,\n",
       "  172.15591430664062,\n",
       "  168.3689422607422,\n",
       "  170.66148376464844,\n",
       "  170.2103729248047,\n",
       "  170.30506896972656,\n",
       "  169.50624084472656,\n",
       "  170.23974609375,\n",
       "  168.85699462890625,\n",
       "  168.33114624023438,\n",
       "  167.501953125,\n",
       "  168.4624786376953,\n",
       "  172.4059600830078,\n",
       "  173.2940216064453,\n",
       "  173.4062042236328,\n",
       "  173.791015625,\n",
       "  174.2235107421875,\n",
       "  173.9204559326172,\n",
       "  174.47503662109375,\n",
       "  174.57347106933594,\n",
       "  174.59356689453125,\n",
       "  174.66168212890625,\n",
       "  174.42160034179688,\n",
       "  174.31866455078125,\n",
       "  174.60626220703125,\n",
       "  174.89669799804688,\n",
       "  174.4910430908203,\n",
       "  175.0537872314453,\n",
       "  175.18226623535156,\n",
       "  175.1734619140625,\n",
       "  175.02389526367188,\n",
       "  175.12051391601562,\n",
       "  175.01206970214844,\n",
       "  174.632568359375,\n",
       "  175.26414489746094,\n",
       "  175.4849853515625,\n",
       "  175.480224609375,\n",
       "  175.29339599609375,\n",
       "  175.21083068847656,\n",
       "  175.5780792236328,\n",
       "  175.5260009765625,\n",
       "  175.4068145751953,\n",
       "  175.16937255859375,\n",
       "  175.23623657226562,\n",
       "  175.4208984375,\n",
       "  175.4337921142578,\n",
       "  175.2372283935547,\n",
       "  175.42996215820312,\n",
       "  175.56936645507812,\n",
       "  175.1316375732422,\n",
       "  175.57225036621094,\n",
       "  175.29360961914062,\n",
       "  175.24273681640625,\n",
       "  175.0690155029297,\n",
       "  175.61695861816406,\n",
       "  175.29588317871094,\n",
       "  175.3735809326172,\n",
       "  175.55738830566406,\n",
       "  174.9833221435547,\n",
       "  175.20863342285156,\n",
       "  175.49961853027344,\n",
       "  174.90345764160156,\n",
       "  174.67486572265625,\n",
       "  174.4416046142578,\n",
       "  174.29725646972656,\n",
       "  174.21034240722656,\n",
       "  174.1293487548828,\n",
       "  173.76068115234375,\n",
       "  173.81529235839844,\n",
       "  173.8272705078125,\n",
       "  173.798583984375,\n",
       "  173.7229461669922,\n",
       "  173.68556213378906,\n",
       "  173.6542510986328,\n",
       "  173.63507080078125,\n",
       "  173.67054748535156,\n",
       "  173.72662353515625,\n",
       "  173.9148712158203,\n",
       "  173.801513671875,\n",
       "  173.77891540527344,\n",
       "  174.10592651367188,\n",
       "  173.99766540527344,\n",
       "  173.95753479003906,\n",
       "  174.1146240234375,\n",
       "  174.03794860839844,\n",
       "  174.05816650390625,\n",
       "  174.1608428955078],\n",
       " 'beta': [0.0,\n",
       "  0.08,\n",
       "  0.16,\n",
       "  0.24,\n",
       "  0.32,\n",
       "  0.4,\n",
       "  0.48,\n",
       "  0.56,\n",
       "  0.64,\n",
       "  0.72,\n",
       "  0.8,\n",
       "  0.88,\n",
       "  0.96,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.08,\n",
       "  0.16,\n",
       "  0.24,\n",
       "  0.32,\n",
       "  0.4,\n",
       "  0.48,\n",
       "  0.56,\n",
       "  0.64,\n",
       "  0.72,\n",
       "  0.8,\n",
       "  0.88,\n",
       "  0.96,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.08,\n",
       "  0.16,\n",
       "  0.24,\n",
       "  0.32,\n",
       "  0.4,\n",
       "  0.48,\n",
       "  0.56,\n",
       "  0.64,\n",
       "  0.72,\n",
       "  0.8,\n",
       "  0.88,\n",
       "  0.96,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.08,\n",
       "  0.16,\n",
       "  0.24,\n",
       "  0.32,\n",
       "  0.4,\n",
       "  0.48,\n",
       "  0.56,\n",
       "  0.64,\n",
       "  0.72,\n",
       "  0.8,\n",
       "  0.88,\n",
       "  0.96,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " 'learning_rate': [0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.01,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taef.train_VAE(\n",
    "    datasets=datasets,\n",
    "    h_dim_1=H_DIM_1,\n",
    "    h_dim_2=H_DIM_2,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    beta=BETA,\n",
    "    model_path=MODEL_PATH,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28e43304-48f4-4653-8ead-0d179c8089d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning evaluation procedure... booting up...\n",
      "Calculating AD scores for each dataset...\n",
      "Evaluating hChToTauNu set...\n",
      "23759/23759 [==============================] - 20s 845us/step\n",
      "Evaluating hToTauTau set...\n",
      "21603/21603 [==============================] - 18s 852us/step\n",
      "Evaluating Ato4l set...\n",
      "1750/1750 [==============================] - 2s 892us/step\n",
      "Evaluating leptoquark set...\n",
      "10642/10642 [==============================] - 9s 873us/step\n",
      "Evaluating train set...\n",
      "62500/62500 [==============================] - 58s 921us/step\n",
      "Evaluating val set...\n",
      "31250/31250 [==============================] - 27s 870us/step\n",
      "Evaluating test set...\n",
      "31250/31250 [==============================] - 27s 866us/step\n",
      "Training student network...\n",
      "Initializing knowledge distillation procedure...\n",
      "Creating student network...\n",
      "Starting training of the student network.\n",
      "Epoch 1/100\n",
      "15625/15625 [==============================] - 37s 2ms/step - loss: 172957827072.0000 - val_loss: 115729866752.0000 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "15625/15625 [==============================] - 36s 2ms/step - loss: 143949103104.0000 - val_loss: 70951591936.0000 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 129971806208.0000 - val_loss: 40079052800.0000 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "15625/15625 [==============================] - 36s 2ms/step - loss: 123816984576.0000 - val_loss: 24890378240.0000 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 119685832704.0000 - val_loss: 26210754560.0000 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 118125346816.0000 - val_loss: 27485526016.0000 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 115594387456.0000 - val_loss: 128948256768.0000 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 114218500096.0000 - val_loss: 235136335872.0000 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 110956961792.0000 - val_loss: 71241662464.0000 - lr: 1.0000e-03\n",
      "Epoch 10/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 109944668160.0000 - val_loss: 221526753280.0000 - lr: 1.0000e-03\n",
      "Epoch 11/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 110034018304.0000 - val_loss: 109989044224.0000 - lr: 1.0000e-03\n",
      "Epoch 12/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 109557514240.0000 - val_loss: 289947680768.0000 - lr: 1.0000e-03\n",
      "Saving student network...\n",
      "Student network saved! Knowledge distillation complete.\n",
      "Running inference on the student network...\n",
      "23759/23759 [==============================] - 15s 637us/step\n",
      "21603/21603 [==============================] - 14s 632us/step\n",
      "1750/1750 [==============================] - 1s 642us/step\n",
      "10642/10642 [==============================] - 7s 687us/step\n",
      "62500/62500 [==============================] - 43s 688us/step\n",
      "31250/31250 [==============================] - 21s 682us/step\n",
      "31250/31250 [==============================] - 21s 675us/step\n",
      "Inference complete! Plotting student performance...\n",
      "Plotting ROC curves...\n",
      "ROC curves plotted! Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "datasets = taef.evaluate_VAE(\n",
    "    datasets=datasets,\n",
    "    model_path=MODEL_PATH,\n",
    "    plots_path=PLOTS_PATH,\n",
    "    h_dim_1=H_DIM_1,\n",
    "    h_dim_2=H_DIM_2,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    beta=BETA,\n",
    "    train_student=True,\n",
    "    load_student=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2164d92d-dab6-49dc-803b-0b55ee7aa2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82.17086029  0.         -1.56314683  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         26.40037155  0.99070734  2.22225761\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 8.2657715e+01  2.7620792e-04 -4.2125699e-01  8.0348969e-02\n",
      "  3.7631989e-03  5.1021576e-03  1.6564220e-02  4.2757764e-04\n",
      "  3.1682253e-03  1.6501595e-03  7.5876713e-05  1.2767315e-04\n",
      "  5.3912401e-05 -2.2144616e-04 -7.5507164e-04  2.7484055e+01\n",
      "  8.5931271e-05  4.1950035e-01  5.1629916e-03 -1.7373338e-03\n",
      " -7.3313713e-05  4.1009486e-04  1.9982457e-05  5.3234398e-05\n",
      " -2.3213029e-04  2.3321807e-04  2.1831095e-03 -1.1568546e-01\n",
      "  7.5326199e-03  1.0698527e-02 -7.1201324e-03  6.9389604e-03\n",
      " -7.1431436e-03 -3.1028336e-01 -3.9177015e-05 -1.9162893e-04\n",
      " -4.9148977e-02  4.1431263e-03 -9.2398375e-04  6.0995221e-03\n",
      " -1.4741160e-03  6.4393654e-03  1.4109820e-02 -1.3158619e-03\n",
      " -2.0958781e-03  3.1015724e-03  5.0300406e-04  1.6493304e-04\n",
      "  5.3979848e-03 -1.9821525e-04  1.1894852e-04  1.7361641e-03\n",
      " -1.8522143e-05 -4.1672587e-04  2.1047378e-03  3.3931434e-04\n",
      "  3.5670400e-04]\n",
      "[ 164.07468  -377.39215    16.425009]\n",
      "[-217.79587 -246.78632 -197.54932]\n"
     ]
    }
   ],
   "source": [
    "tag = 'test'\n",
    "idx=0\n",
    "print(datasets[tag]['data'][idx])\n",
    "print(datasets[tag]['y_pred'][idx])\n",
    "print(datasets[tag]['z_mean'][idx])\n",
    "print(datasets[tag]['z_log_var'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6573b0-921b-487a-b0ca-324868264ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.12.0",
   "language": "python",
   "name": "tensorflow-2.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
