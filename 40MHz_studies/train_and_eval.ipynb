{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b88cb9-4c3e-497c-be72-020a1a83cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import train_and_eval_functions as taef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2278055a-5489-4a99-843a-ff2d061729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 0.7\n",
    "MODEL_PATH = '/global/homes/m/mcohen54/chop_or_not_development/trained_models/trial_6'\n",
    "PLOTS_PATH = MODEL_PATH+'/plots'\n",
    "H_DIM_1 = 16\n",
    "H_DIM_2 = 8\n",
    "LATENT_DIM = 3\n",
    "# ANNEALING_TYPE = 'cyclical'\n",
    "# WARMUP_EPOCHS=None \n",
    "# INCREASE_EPOCHS=None\n",
    "ANNEALING_TYPE = 'standard'\n",
    "WARMUP_EPOCHS=15\n",
    "INCREASE_EPOCHS=25\n",
    "LEARNING_RATES = [0.005, 0.002, 0.001, 0.0005, 0.0001]\n",
    "STAGE_LENGTHS = [20, 20, 25, 20, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd620c0-4582-4e75-ae51-9774dcb2b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booting up...\n",
      "Starting to load data...\n",
      "\n",
      "Loading hChToTauNu_13TeV_PU20.h5...\n",
      "Loading hToTauTau_13TeV_PU20.h5...\n",
      "Loading Ato4l_lepFilter_13TeV.h5...\n",
      "Loading background_for_training.h5...\n",
      "Loading leptoquark_LOWMASS_lepFilter_13TeV.h5...\n",
      "Beginning preprocessing...\n",
      "\n",
      "Load and preprocessing complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = taef.load_and_preprocess(data_path='/global/homes/m/mcohen54/chop_or_not_development/data', standard_scaler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ca4de2-b107-44c4-914a-c027caa09fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training procedure... booting up...\n",
      "Using standard beta annealing schedule\n",
      "Standard schedule phases:\n",
      "  - Warmup (β=0): 15 epochs\n",
      "  - Increase (β=0→0.7): 25 epochs\n",
      "  - Constant (β=0.7): 60 epochs\n",
      "\n",
      "Learning rate schedule (5 stages):\n",
      "  - Stage 1: epochs   0- 19, lr = 0.005000\n",
      "  - Stage 2: epochs  20- 39, lr = 0.002000\n",
      "  - Stage 3: epochs  40- 64, lr = 0.001000\n",
      "  - Stage 4: epochs  65- 84, lr = 0.000500\n",
      "  - Stage 5: epochs  85- 99, lr = 0.000100\n",
      "\n",
      "Initialization attempt 1/10\n",
      "Successfully initialized network with valid losses!\n",
      "\n",
      "Starting training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 03:14:48.703608: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [2000000,57]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN detected in mean!\n",
      "mean stats: Tensor(\"cond/Min:0\", shape=(), dtype=float32) Tensor(\"cond/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in log_var!\n",
      "log_var stats: Tensor(\"cond_1/Min:0\", shape=(), dtype=float32) Tensor(\"cond_1/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in sampled z!\n",
      "z stats: Tensor(\"cond_2/Min:0\", shape=(), dtype=float32) Tensor(\"cond_2/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in mean!\n",
      "mean stats: Tensor(\"cond/Min:0\", shape=(), dtype=float32) Tensor(\"cond/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in log_var!\n",
      "log_var stats: Tensor(\"cond_1/Min:0\", shape=(), dtype=float32) Tensor(\"cond_1/Max:0\", shape=(), dtype=float32)\n",
      "NaN detected in sampled z!\n",
      "z stats: Tensor(\"cond_2/Min:0\", shape=(), dtype=float32) Tensor(\"cond_2/Max:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 03:15:33.946597: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [1000000,57]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 33463.2734 - Reconstruction: 33463.2734 - KL: 50000088268800.0000\n",
      "Val Total Loss: 110.8525 - Val Reconstruction: 110.8525 - Val KL: 175.3036\n",
      "Epoch 2/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 287.4412 - Reconstruction: 287.4412 - KL: 6054910.0000\n",
      "Val Total Loss: 100.6449 - Val Reconstruction: 100.6449 - Val KL: 169.3530\n",
      "Epoch 3/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 96.3166 - Reconstruction: 96.3166 - KL: 170.1010\n",
      "Val Total Loss: 93.6868 - Val Reconstruction: 93.6868 - Val KL: 171.3736\n",
      "Epoch 4/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 96.9294 - Reconstruction: 96.9294 - KL: 172.3249\n",
      "Val Total Loss: 94.1683 - Val Reconstruction: 94.1683 - Val KL: 172.7078\n",
      "Epoch 5/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 95.1847 - Reconstruction: 95.1847 - KL: 172.6141\n",
      "Val Total Loss: 89.9529 - Val Reconstruction: 89.9529 - Val KL: 172.1546\n",
      "Epoch 6/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 90.2684 - Reconstruction: 90.2684 - KL: 171.7742\n",
      "Val Total Loss: 94.8466 - Val Reconstruction: 94.8466 - Val KL: 172.4905\n",
      "Epoch 7/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 88.3901 - Reconstruction: 88.3901 - KL: 171.8735\n",
      "Val Total Loss: 85.2337 - Val Reconstruction: 85.2337 - Val KL: 172.5121\n",
      "Epoch 8/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 78.1732 - Reconstruction: 78.1732 - KL: 172.7774\n",
      "Val Total Loss: 73.5705 - Val Reconstruction: 73.5705 - Val KL: 173.2223\n",
      "Epoch 9/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 65.3907 - Reconstruction: 65.3907 - KL: 174.1568\n",
      "Val Total Loss: 64.5966 - Val Reconstruction: 64.5966 - Val KL: 174.5012\n",
      "Epoch 10/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 62.6409 - Reconstruction: 62.6409 - KL: 174.9536\n",
      "Val Total Loss: 60.7515 - Val Reconstruction: 60.7515 - Val KL: 175.1782\n",
      "Epoch 11/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 62.3795 - Reconstruction: 62.3795 - KL: 172.5926\n",
      "Val Total Loss: 61.3628 - Val Reconstruction: 61.3628 - Val KL: 168.0076\n",
      "Epoch 12/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 61.6314 - Reconstruction: 61.6314 - KL: 167.9173\n",
      "Val Total Loss: 60.3534 - Val Reconstruction: 60.3534 - Val KL: 168.2045\n",
      "Epoch 13/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 61.1208 - Reconstruction: 61.1208 - KL: 167.9280\n",
      "Val Total Loss: 60.9826 - Val Reconstruction: 60.9826 - Val KL: 167.3410\n",
      "Epoch 14/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 60.5496 - Reconstruction: 60.5496 - KL: 167.3340\n",
      "Val Total Loss: 58.2662 - Val Reconstruction: 58.2662 - Val KL: 168.0111\n",
      "Epoch 15/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 59.9518 - Reconstruction: 59.9518 - KL: 167.2352\n",
      "Val Total Loss: 58.7976 - Val Reconstruction: 58.7976 - Val KL: 166.3068\n",
      "Epoch 16/100\n",
      "Beta: 0.0000, Learning Rate: 0.005000\n",
      "Total Loss: 59.4674 - Reconstruction: 59.4674 - KL: 166.5213\n",
      "Val Total Loss: 58.7183 - Val Reconstruction: 58.7183 - Val KL: 166.0228\n",
      "Epoch 17/100\n",
      "Beta: 0.0280, Learning Rate: 0.005000\n",
      "Total Loss: 60.7492 - Reconstruction: 60.7492 - KL: 165.9129\n",
      "Val Total Loss: 63.5277 - Val Reconstruction: 58.8525 - Val KL: 166.9710\n",
      "Epoch 18/100\n",
      "Beta: 0.0560, Learning Rate: 0.005000\n",
      "Total Loss: 59.7204 - Reconstruction: 59.7204 - KL: 166.7039\n",
      "Val Total Loss: 67.5212 - Val Reconstruction: 58.1430 - Val KL: 167.4667\n",
      "Epoch 19/100\n",
      "Beta: 0.0840, Learning Rate: 0.005000\n",
      "Total Loss: 75.1287 - Reconstruction: 75.1287 - KL: 167.0755\n",
      "Val Total Loss: 118.2577 - Val Reconstruction: 104.4171 - Val KL: 164.7689\n",
      "Epoch 20/100\n",
      "Beta: 0.1120, Learning Rate: 0.005000\n",
      "Total Loss: 100.2119 - Reconstruction: 100.2119 - KL: 164.4171\n",
      "Val Total Loss: 115.7256 - Val Reconstruction: 97.1343 - Val KL: 165.9937\n",
      "Epoch 21/100\n",
      "Beta: 0.1400, Learning Rate: 0.002000\n",
      "Total Loss: 92.8688 - Reconstruction: 92.8688 - KL: 168.1311\n",
      "Val Total Loss: 113.5090 - Val Reconstruction: 89.5668 - Val KL: 171.0158\n",
      "Epoch 22/100\n",
      "Beta: 0.1680, Learning Rate: 0.002000\n",
      "Total Loss: 92.1026 - Reconstruction: 92.1026 - KL: 170.6458\n",
      "Val Total Loss: 114.9024 - Val Reconstruction: 85.8051 - Val KL: 173.1983\n",
      "Epoch 23/100\n",
      "Beta: 0.1960, Learning Rate: 0.002000\n",
      "Total Loss: 82.8032 - Reconstruction: 82.8032 - KL: 169.7329\n",
      "Val Total Loss: 112.8888 - Val Reconstruction: 79.7833 - Val KL: 168.9059\n",
      "Epoch 24/100\n",
      "Beta: 0.2240, Learning Rate: 0.002000\n",
      "Total Loss: 77.6486 - Reconstruction: 77.6486 - KL: 167.2358\n",
      "Val Total Loss: 116.0429 - Val Reconstruction: 78.6033 - Val KL: 167.1407\n",
      "Epoch 25/100\n",
      "Beta: 0.2520, Learning Rate: 0.002000\n",
      "Total Loss: 75.0220 - Reconstruction: 75.0220 - KL: 166.2450\n",
      "Val Total Loss: 119.9956 - Val Reconstruction: 78.1515 - Val KL: 166.0481\n",
      "Epoch 26/100\n",
      "Beta: 0.2800, Learning Rate: 0.002000\n",
      "Total Loss: 76.0492 - Reconstruction: 76.0492 - KL: 166.2210\n",
      "Val Total Loss: 124.8190 - Val Reconstruction: 78.2062 - Val KL: 166.4743\n",
      "Epoch 27/100\n",
      "Beta: 0.3080, Learning Rate: 0.002000\n",
      "Total Loss: 74.2925 - Reconstruction: 74.2925 - KL: 166.2030\n",
      "Val Total Loss: 124.2596 - Val Reconstruction: 73.0688 - Val KL: 166.2037\n",
      "Epoch 28/100\n",
      "Beta: 0.3360, Learning Rate: 0.002000\n",
      "Total Loss: 77.1247 - Reconstruction: 77.1247 - KL: 166.3281\n",
      "Val Total Loss: 142.4381 - Val Reconstruction: 86.3130 - Val KL: 167.0387\n",
      "Epoch 29/100\n",
      "Beta: 0.3640, Learning Rate: 0.002000\n",
      "Total Loss: 84.3015 - Reconstruction: 84.3015 - KL: 166.9321\n",
      "Val Total Loss: 144.7192 - Val Reconstruction: 84.0123 - Val KL: 166.7773\n",
      "Epoch 30/100\n",
      "Beta: 0.3920, Learning Rate: 0.002000\n",
      "Total Loss: 134.7998 - Reconstruction: 134.7998 - KL: 169.2476\n",
      "Val Total Loss: 206.5590 - Val Reconstruction: 139.2091 - Val KL: 171.8110\n",
      "Epoch 31/100\n",
      "Beta: 0.4200, Learning Rate: 0.002000\n",
      "Total Loss: 98.2017 - Reconstruction: 98.2017 - KL: 171.9311\n",
      "Val Total Loss: 160.6178 - Val Reconstruction: 88.4101 - Val KL: 171.9231\n",
      "Epoch 32/100\n",
      "Beta: 0.4480, Learning Rate: 0.002000\n",
      "Total Loss: 84.1244 - Reconstruction: 84.1244 - KL: 173.2176\n",
      "Val Total Loss: 160.0877 - Val Reconstruction: 81.9728 - Val KL: 174.3638\n",
      "Epoch 33/100\n",
      "Beta: 0.4760, Learning Rate: 0.002000\n",
      "Total Loss: 77.8483 - Reconstruction: 77.8483 - KL: 174.6242\n",
      "Val Total Loss: 157.5436 - Val Reconstruction: 74.4381 - Val KL: 174.5913\n",
      "Epoch 34/100\n",
      "Beta: 0.5040, Learning Rate: 0.002000\n",
      "Total Loss: 71.9947 - Reconstruction: 71.9947 - KL: 174.8644\n",
      "Val Total Loss: 158.0282 - Val Reconstruction: 69.5555 - Val KL: 175.5411\n",
      "Epoch 35/100\n",
      "Beta: 0.5320, Learning Rate: 0.002000\n",
      "Total Loss: 67.0928 - Reconstruction: 67.0928 - KL: 176.3468\n",
      "Val Total Loss: 157.9187 - Val Reconstruction: 63.7553 - Val KL: 176.9989\n",
      "Epoch 36/100\n",
      "Beta: 0.5600, Learning Rate: 0.002000\n",
      "Total Loss: 68.4355 - Reconstruction: 68.4355 - KL: 177.0511\n",
      "Val Total Loss: 169.2307 - Val Reconstruction: 70.1286 - Val KL: 176.9679\n",
      "Epoch 37/100\n",
      "Beta: 0.5880, Learning Rate: 0.002000\n",
      "Total Loss: 68.6147 - Reconstruction: 68.6147 - KL: 176.7097\n",
      "Val Total Loss: 168.6219 - Val Reconstruction: 64.8059 - Val KL: 176.5578\n",
      "Epoch 38/100\n",
      "Beta: 0.6160, Learning Rate: 0.002000\n",
      "Total Loss: 68.2697 - Reconstruction: 68.2697 - KL: 176.5397\n",
      "Val Total Loss: 177.7686 - Val Reconstruction: 68.9884 - Val KL: 176.5913\n",
      "Epoch 39/100\n",
      "Beta: 0.6440, Learning Rate: 0.002000\n",
      "Total Loss: 67.8325 - Reconstruction: 67.8325 - KL: 176.5693\n",
      "Val Total Loss: 182.4551 - Val Reconstruction: 68.7698 - Val KL: 176.5300\n",
      "Epoch 40/100\n",
      "Beta: 0.6720, Learning Rate: 0.002000\n",
      "Total Loss: 68.1800 - Reconstruction: 68.1800 - KL: 176.4629\n",
      "Val Total Loss: 185.6580 - Val Reconstruction: 67.1170 - Val KL: 176.4003\n",
      "Epoch 41/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 67.2069 - Reconstruction: 67.2069 - KL: 176.4560\n",
      "Val Total Loss: 191.3725 - Val Reconstruction: 67.8031 - Val KL: 176.5277\n",
      "Epoch 42/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 67.7988 - Reconstruction: 67.7988 - KL: 176.4895\n",
      "Val Total Loss: 189.2822 - Val Reconstruction: 65.7708 - Val KL: 176.4448\n",
      "Epoch 43/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.7674 - Reconstruction: 65.7674 - KL: 176.4857\n",
      "Val Total Loss: 187.9268 - Val Reconstruction: 64.3814 - Val KL: 176.4933\n",
      "Epoch 44/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 67.2624 - Reconstruction: 67.2624 - KL: 176.5527\n",
      "Val Total Loss: 188.3259 - Val Reconstruction: 64.8242 - Val KL: 176.4311\n",
      "Epoch 45/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 64.8603 - Reconstruction: 64.8603 - KL: 176.5153\n",
      "Val Total Loss: 186.8204 - Val Reconstruction: 63.2893 - Val KL: 176.4729\n",
      "Epoch 46/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.7547 - Reconstruction: 65.7547 - KL: 176.5285\n",
      "Val Total Loss: 190.7680 - Val Reconstruction: 67.2400 - Val KL: 176.4687\n",
      "Epoch 47/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 66.5037 - Reconstruction: 66.5037 - KL: 176.4503\n",
      "Val Total Loss: 189.7406 - Val Reconstruction: 66.2319 - Val KL: 176.4410\n",
      "Epoch 48/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.3200 - Reconstruction: 65.3200 - KL: 176.4274\n",
      "Val Total Loss: 188.5581 - Val Reconstruction: 65.1841 - Val KL: 176.2486\n",
      "Epoch 49/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.8644 - Reconstruction: 65.8644 - KL: 176.3158\n",
      "Val Total Loss: 191.0044 - Val Reconstruction: 67.6590 - Val KL: 176.2077\n",
      "Epoch 50/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.6407 - Reconstruction: 65.6407 - KL: 175.9954\n",
      "Val Total Loss: 186.9730 - Val Reconstruction: 64.0800 - Val KL: 175.5614\n",
      "Epoch 51/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 64.6307 - Reconstruction: 64.6307 - KL: 175.5424\n",
      "Val Total Loss: 186.4496 - Val Reconstruction: 63.7792 - Val KL: 175.2435\n",
      "Epoch 52/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.5059 - Reconstruction: 65.5059 - KL: 174.6831\n",
      "Val Total Loss: 188.2559 - Val Reconstruction: 66.4563 - Val KL: 173.9995\n",
      "Epoch 53/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.1857 - Reconstruction: 65.1857 - KL: 173.9824\n",
      "Val Total Loss: 185.4000 - Val Reconstruction: 63.6633 - Val KL: 173.9096\n",
      "Epoch 54/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.4424 - Reconstruction: 65.4424 - KL: 173.2744\n",
      "Val Total Loss: 187.6487 - Val Reconstruction: 66.6018 - Val KL: 172.9241\n",
      "Epoch 55/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 66.0877 - Reconstruction: 66.0877 - KL: 172.8483\n",
      "Val Total Loss: 184.0082 - Val Reconstruction: 63.1975 - Val KL: 172.5867\n",
      "Epoch 56/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 64.4032 - Reconstruction: 64.4032 - KL: 172.6818\n",
      "Val Total Loss: 186.3378 - Val Reconstruction: 65.3492 - Val KL: 172.8409\n",
      "Epoch 57/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.9057 - Reconstruction: 65.9057 - KL: 172.8844\n",
      "Val Total Loss: 188.4540 - Val Reconstruction: 67.1951 - Val KL: 173.2270\n",
      "Epoch 58/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 66.3503 - Reconstruction: 66.3503 - KL: 173.0096\n",
      "Val Total Loss: 186.3655 - Val Reconstruction: 65.1697 - Val KL: 173.1369\n",
      "Epoch 59/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 65.1082 - Reconstruction: 65.1082 - KL: 173.1000\n",
      "Val Total Loss: 187.0437 - Val Reconstruction: 65.6591 - Val KL: 173.4066\n",
      "Epoch 60/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 64.1778 - Reconstruction: 64.1778 - KL: 173.1883\n",
      "Val Total Loss: 186.8544 - Val Reconstruction: 65.3541 - Val KL: 173.5719\n",
      "Epoch 61/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 63.8189 - Reconstruction: 63.8189 - KL: 173.1472\n",
      "Val Total Loss: 184.4871 - Val Reconstruction: 63.1750 - Val KL: 173.3031\n",
      "Epoch 62/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 64.9636 - Reconstruction: 64.9636 - KL: 173.4787\n",
      "Val Total Loss: 187.3429 - Val Reconstruction: 65.6963 - Val KL: 173.7809\n",
      "Epoch 63/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 64.5103 - Reconstruction: 64.5103 - KL: 173.4799\n",
      "Val Total Loss: 185.1736 - Val Reconstruction: 63.7493 - Val KL: 173.4633\n",
      "Epoch 64/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 64.2617 - Reconstruction: 64.2617 - KL: 173.4190\n",
      "Val Total Loss: 186.7900 - Val Reconstruction: 65.1986 - Val KL: 173.7021\n",
      "Epoch 65/100\n",
      "Beta: 0.7000, Learning Rate: 0.001000\n",
      "Total Loss: 63.7278 - Reconstruction: 63.7278 - KL: 173.4008\n",
      "Val Total Loss: 186.3486 - Val Reconstruction: 64.8075 - Val KL: 173.6300\n",
      "Epoch 66/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 62.8559 - Reconstruction: 62.8559 - KL: 173.4500\n",
      "Val Total Loss: 185.1352 - Val Reconstruction: 63.6429 - Val KL: 173.5604\n",
      "Epoch 67/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 63.9738 - Reconstruction: 63.9738 - KL: 173.6826\n",
      "Val Total Loss: 186.1149 - Val Reconstruction: 64.4499 - Val KL: 173.8072\n",
      "Epoch 68/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 63.9473 - Reconstruction: 63.9473 - KL: 173.7754\n",
      "Val Total Loss: 185.8313 - Val Reconstruction: 64.1402 - Val KL: 173.8445\n",
      "Epoch 69/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 64.2758 - Reconstruction: 64.2758 - KL: 173.8907\n",
      "Val Total Loss: 186.4247 - Val Reconstruction: 64.6769 - Val KL: 173.9255\n",
      "Epoch 70/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 64.5209 - Reconstruction: 64.5209 - KL: 173.9905\n",
      "Val Total Loss: 188.1608 - Val Reconstruction: 66.2369 - Val KL: 174.1770\n",
      "Epoch 71/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 65.1826 - Reconstruction: 65.1826 - KL: 174.1044\n",
      "Val Total Loss: 188.1257 - Val Reconstruction: 66.2321 - Val KL: 174.1339\n",
      "Epoch 72/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 64.7673 - Reconstruction: 64.7673 - KL: 174.0507\n",
      "Val Total Loss: 187.6967 - Val Reconstruction: 65.7745 - Val KL: 174.1747\n",
      "Epoch 73/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 65.1733 - Reconstruction: 65.1733 - KL: 174.1044\n",
      "Val Total Loss: 188.6069 - Val Reconstruction: 66.6689 - Val KL: 174.1972\n",
      "Epoch 74/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 65.0574 - Reconstruction: 65.0574 - KL: 174.1078\n",
      "Val Total Loss: 186.4753 - Val Reconstruction: 64.6049 - Val KL: 174.1005\n",
      "Epoch 75/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 63.9745 - Reconstruction: 63.9745 - KL: 174.0820\n",
      "Val Total Loss: 187.6417 - Val Reconstruction: 65.7508 - Val KL: 174.1299\n",
      "Epoch 76/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 64.5351 - Reconstruction: 64.5351 - KL: 174.1693\n",
      "Val Total Loss: 187.7964 - Val Reconstruction: 65.7750 - Val KL: 174.3162\n",
      "Epoch 77/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 65.6874 - Reconstruction: 65.6874 - KL: 174.2996\n",
      "Val Total Loss: 188.1534 - Val Reconstruction: 66.1133 - Val KL: 174.3431\n",
      "Epoch 78/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 64.3360 - Reconstruction: 64.3360 - KL: 174.1929\n",
      "Val Total Loss: 186.7676 - Val Reconstruction: 64.8019 - Val KL: 174.2366\n",
      "Epoch 79/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 64.8846 - Reconstruction: 64.8846 - KL: 174.2891\n",
      "Val Total Loss: 188.1636 - Val Reconstruction: 66.0969 - Val KL: 174.3810\n",
      "Epoch 80/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 64.0386 - Reconstruction: 64.0386 - KL: 174.2258\n",
      "Val Total Loss: 186.8472 - Val Reconstruction: 64.8694 - Val KL: 174.2540\n",
      "Epoch 81/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 63.6342 - Reconstruction: 63.6342 - KL: 174.2155\n",
      "Val Total Loss: 187.0400 - Val Reconstruction: 65.0201 - Val KL: 174.3142\n",
      "Epoch 82/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 63.9727 - Reconstruction: 63.9727 - KL: 174.2554\n",
      "Val Total Loss: 186.1719 - Val Reconstruction: 64.2247 - Val KL: 174.2102\n",
      "Epoch 83/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 63.2286 - Reconstruction: 63.2286 - KL: 174.1855\n",
      "Val Total Loss: 186.1579 - Val Reconstruction: 64.1649 - Val KL: 174.2757\n",
      "Epoch 84/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 64.0149 - Reconstruction: 64.0149 - KL: 174.2563\n",
      "Val Total Loss: 187.1661 - Val Reconstruction: 65.1332 - Val KL: 174.3327\n",
      "Epoch 85/100\n",
      "Beta: 0.7000, Learning Rate: 0.000500\n",
      "Total Loss: 64.4061 - Reconstruction: 64.4061 - KL: 174.2926\n",
      "Val Total Loss: 187.5236 - Val Reconstruction: 65.4438 - Val KL: 174.3997\n",
      "Epoch 86/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 64.2178 - Reconstruction: 64.2178 - KL: 174.3295\n",
      "Val Total Loss: 186.7691 - Val Reconstruction: 64.7518 - Val KL: 174.3104\n",
      "Epoch 87/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 64.1947 - Reconstruction: 64.1947 - KL: 174.3176\n",
      "Val Total Loss: 186.8252 - Val Reconstruction: 64.7884 - Val KL: 174.3383\n",
      "Epoch 88/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 64.2453 - Reconstruction: 64.2453 - KL: 174.3538\n",
      "Val Total Loss: 186.9687 - Val Reconstruction: 64.9143 - Val KL: 174.3634\n",
      "Epoch 89/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 64.0449 - Reconstruction: 64.0449 - KL: 174.3731\n",
      "Val Total Loss: 186.9741 - Val Reconstruction: 64.8750 - Val KL: 174.4273\n",
      "Epoch 90/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 64.4441 - Reconstruction: 64.4441 - KL: 174.4515\n",
      "Val Total Loss: 187.7301 - Val Reconstruction: 65.5770 - Val KL: 174.5045\n",
      "Epoch 91/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 64.7327 - Reconstruction: 64.7327 - KL: 174.5084\n",
      "Val Total Loss: 187.8031 - Val Reconstruction: 65.6358 - Val KL: 174.5248\n",
      "Epoch 92/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 64.3054 - Reconstruction: 64.3054 - KL: 174.5208\n",
      "Val Total Loss: 187.2052 - Val Reconstruction: 65.0215 - Val KL: 174.5482\n",
      "Epoch 93/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 64.1545 - Reconstruction: 64.1545 - KL: 174.5441\n",
      "Val Total Loss: 187.1680 - Val Reconstruction: 64.9764 - Val KL: 174.5594\n",
      "Epoch 94/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 63.8978 - Reconstruction: 63.8978 - KL: 174.5515\n",
      "Val Total Loss: 186.9999 - Val Reconstruction: 64.8153 - Val KL: 174.5495\n",
      "Epoch 95/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 63.6433 - Reconstruction: 63.6433 - KL: 174.5611\n",
      "Val Total Loss: 186.7191 - Val Reconstruction: 64.5190 - Val KL: 174.5716\n",
      "Epoch 96/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 63.4107 - Reconstruction: 63.4107 - KL: 174.5716\n",
      "Val Total Loss: 186.5460 - Val Reconstruction: 64.3437 - Val KL: 174.5747\n",
      "Epoch 97/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 63.3893 - Reconstruction: 63.3893 - KL: 174.5949\n",
      "Val Total Loss: 186.5869 - Val Reconstruction: 64.3691 - Val KL: 174.5968\n",
      "Epoch 98/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 63.4704 - Reconstruction: 63.4704 - KL: 174.6222\n",
      "Val Total Loss: 186.7620 - Val Reconstruction: 64.4985 - Val KL: 174.6622\n",
      "Epoch 99/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 63.4226 - Reconstruction: 63.4226 - KL: 174.6551\n",
      "Val Total Loss: 186.7023 - Val Reconstruction: 64.4432 - Val KL: 174.6559\n",
      "Epoch 100/100\n",
      "Beta: 0.7000, Learning Rate: 0.000100\n",
      "Total Loss: 63.4905 - Reconstruction: 63.4905 - KL: 174.6684\n",
      "Val Total Loss: 186.7026 - Val Reconstruction: 64.4284 - Val KL: 174.6773\n",
      "Training complete!\n",
      "Saving model...\n",
      "Training history plots saved to /global/homes/m/mcohen54/chop_or_not_development/trained_models/trial_6/training_history.png\n",
      "Model saved! Powering down...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_loss': [33463.2734375,\n",
       "  287.44122314453125,\n",
       "  96.31660461425781,\n",
       "  96.92941284179688,\n",
       "  95.18465423583984,\n",
       "  90.26837158203125,\n",
       "  88.39014434814453,\n",
       "  78.17320251464844,\n",
       "  65.39067077636719,\n",
       "  62.64091110229492,\n",
       "  62.379539489746094,\n",
       "  61.63142013549805,\n",
       "  61.120765686035156,\n",
       "  60.549644470214844,\n",
       "  59.95178985595703,\n",
       "  59.46735382080078,\n",
       "  60.74921417236328,\n",
       "  59.7204475402832,\n",
       "  75.12870025634766,\n",
       "  100.21190643310547,\n",
       "  92.86881256103516,\n",
       "  92.1025619506836,\n",
       "  82.8032455444336,\n",
       "  77.64859008789062,\n",
       "  75.02196502685547,\n",
       "  76.04924774169922,\n",
       "  74.29254913330078,\n",
       "  77.12467193603516,\n",
       "  84.30145263671875,\n",
       "  134.7998046875,\n",
       "  98.20174407958984,\n",
       "  84.12439727783203,\n",
       "  77.84831237792969,\n",
       "  71.99472045898438,\n",
       "  67.09282684326172,\n",
       "  68.43553924560547,\n",
       "  68.61470031738281,\n",
       "  68.26972961425781,\n",
       "  67.83247375488281,\n",
       "  68.18002319335938,\n",
       "  67.20689392089844,\n",
       "  67.79881286621094,\n",
       "  65.76736450195312,\n",
       "  67.26240539550781,\n",
       "  64.86029052734375,\n",
       "  65.75468444824219,\n",
       "  66.503662109375,\n",
       "  65.3199691772461,\n",
       "  65.86444091796875,\n",
       "  65.64070892333984,\n",
       "  64.63069152832031,\n",
       "  65.50592041015625,\n",
       "  65.18565368652344,\n",
       "  65.4423828125,\n",
       "  66.08766174316406,\n",
       "  64.40320587158203,\n",
       "  65.9056625366211,\n",
       "  66.35031127929688,\n",
       "  65.10823822021484,\n",
       "  64.1778335571289,\n",
       "  63.81892395019531,\n",
       "  64.96363830566406,\n",
       "  64.51026916503906,\n",
       "  64.26171112060547,\n",
       "  63.72775650024414,\n",
       "  62.85586929321289,\n",
       "  63.97383499145508,\n",
       "  63.947269439697266,\n",
       "  64.27581024169922,\n",
       "  64.52088165283203,\n",
       "  65.18260192871094,\n",
       "  64.76725006103516,\n",
       "  65.17326354980469,\n",
       "  65.05736541748047,\n",
       "  63.97453308105469,\n",
       "  64.53506469726562,\n",
       "  65.68740844726562,\n",
       "  64.33604431152344,\n",
       "  64.8846206665039,\n",
       "  64.03858947753906,\n",
       "  63.6341667175293,\n",
       "  63.97270965576172,\n",
       "  63.22858810424805,\n",
       "  64.0148696899414,\n",
       "  64.40608978271484,\n",
       "  64.21784973144531,\n",
       "  64.19471740722656,\n",
       "  64.24530029296875,\n",
       "  64.04486083984375,\n",
       "  64.44406127929688,\n",
       "  64.73274230957031,\n",
       "  64.30538940429688,\n",
       "  64.1545181274414,\n",
       "  63.89776611328125,\n",
       "  63.64331817626953,\n",
       "  63.41071319580078,\n",
       "  63.38928985595703,\n",
       "  63.47043991088867,\n",
       "  63.42258071899414,\n",
       "  63.490535736083984],\n",
       " 'reconstruction_loss': [33463.2734375,\n",
       "  287.44122314453125,\n",
       "  96.31660461425781,\n",
       "  96.92941284179688,\n",
       "  95.18465423583984,\n",
       "  90.26837158203125,\n",
       "  88.39014434814453,\n",
       "  78.17320251464844,\n",
       "  65.39067077636719,\n",
       "  62.64091110229492,\n",
       "  62.379539489746094,\n",
       "  61.63142013549805,\n",
       "  61.120765686035156,\n",
       "  60.549644470214844,\n",
       "  59.95178985595703,\n",
       "  59.46735382080078,\n",
       "  60.74921417236328,\n",
       "  59.7204475402832,\n",
       "  75.12870025634766,\n",
       "  100.21190643310547,\n",
       "  92.86881256103516,\n",
       "  92.1025619506836,\n",
       "  82.8032455444336,\n",
       "  77.64859008789062,\n",
       "  75.02196502685547,\n",
       "  76.04924774169922,\n",
       "  74.29254913330078,\n",
       "  77.12467193603516,\n",
       "  84.30145263671875,\n",
       "  134.7998046875,\n",
       "  98.20174407958984,\n",
       "  84.12439727783203,\n",
       "  77.84831237792969,\n",
       "  71.99472045898438,\n",
       "  67.09282684326172,\n",
       "  68.43553924560547,\n",
       "  68.61470031738281,\n",
       "  68.26972961425781,\n",
       "  67.83247375488281,\n",
       "  68.18002319335938,\n",
       "  67.20689392089844,\n",
       "  67.79881286621094,\n",
       "  65.76736450195312,\n",
       "  67.26240539550781,\n",
       "  64.86029052734375,\n",
       "  65.75468444824219,\n",
       "  66.503662109375,\n",
       "  65.3199691772461,\n",
       "  65.86444091796875,\n",
       "  65.64070892333984,\n",
       "  64.63069152832031,\n",
       "  65.50592041015625,\n",
       "  65.18565368652344,\n",
       "  65.4423828125,\n",
       "  66.08766174316406,\n",
       "  64.40320587158203,\n",
       "  65.9056625366211,\n",
       "  66.35031127929688,\n",
       "  65.10823822021484,\n",
       "  64.1778335571289,\n",
       "  63.81892395019531,\n",
       "  64.96363830566406,\n",
       "  64.51026916503906,\n",
       "  64.26171112060547,\n",
       "  63.72775650024414,\n",
       "  62.85586929321289,\n",
       "  63.97383499145508,\n",
       "  63.947269439697266,\n",
       "  64.27581024169922,\n",
       "  64.52088165283203,\n",
       "  65.18260192871094,\n",
       "  64.76725006103516,\n",
       "  65.17326354980469,\n",
       "  65.05736541748047,\n",
       "  63.97453308105469,\n",
       "  64.53506469726562,\n",
       "  65.68740844726562,\n",
       "  64.33604431152344,\n",
       "  64.8846206665039,\n",
       "  64.03858947753906,\n",
       "  63.6341667175293,\n",
       "  63.97270965576172,\n",
       "  63.22858810424805,\n",
       "  64.0148696899414,\n",
       "  64.40608978271484,\n",
       "  64.21784973144531,\n",
       "  64.19471740722656,\n",
       "  64.24530029296875,\n",
       "  64.04486083984375,\n",
       "  64.44406127929688,\n",
       "  64.73274230957031,\n",
       "  64.30538940429688,\n",
       "  64.1545181274414,\n",
       "  63.89776611328125,\n",
       "  63.64331817626953,\n",
       "  63.41071319580078,\n",
       "  63.38928985595703,\n",
       "  63.47043991088867,\n",
       "  63.42258071899414,\n",
       "  63.490535736083984],\n",
       " 'kl_loss': [50000088268800.0,\n",
       "  6054910.0,\n",
       "  170.1010284423828,\n",
       "  172.32489013671875,\n",
       "  172.61410522460938,\n",
       "  171.77420043945312,\n",
       "  171.87350463867188,\n",
       "  172.7773895263672,\n",
       "  174.15675354003906,\n",
       "  174.95358276367188,\n",
       "  172.59255981445312,\n",
       "  167.91726684570312,\n",
       "  167.92800903320312,\n",
       "  167.33396911621094,\n",
       "  167.2351837158203,\n",
       "  166.5212860107422,\n",
       "  165.91285705566406,\n",
       "  166.7039337158203,\n",
       "  167.07545471191406,\n",
       "  164.41708374023438,\n",
       "  168.13113403320312,\n",
       "  170.64584350585938,\n",
       "  169.7328643798828,\n",
       "  167.23583984375,\n",
       "  166.24496459960938,\n",
       "  166.2209930419922,\n",
       "  166.20297241210938,\n",
       "  166.32814025878906,\n",
       "  166.93209838867188,\n",
       "  169.24761962890625,\n",
       "  171.9310760498047,\n",
       "  173.2176055908203,\n",
       "  174.6241912841797,\n",
       "  174.8643798828125,\n",
       "  176.3467559814453,\n",
       "  177.05113220214844,\n",
       "  176.70968627929688,\n",
       "  176.5397186279297,\n",
       "  176.56930541992188,\n",
       "  176.46287536621094,\n",
       "  176.45599365234375,\n",
       "  176.48953247070312,\n",
       "  176.48570251464844,\n",
       "  176.552734375,\n",
       "  176.5152587890625,\n",
       "  176.52854919433594,\n",
       "  176.4502716064453,\n",
       "  176.42739868164062,\n",
       "  176.3158416748047,\n",
       "  175.99537658691406,\n",
       "  175.5424346923828,\n",
       "  174.68310546875,\n",
       "  173.982421875,\n",
       "  173.2743682861328,\n",
       "  172.8483428955078,\n",
       "  172.6817626953125,\n",
       "  172.88438415527344,\n",
       "  173.0095977783203,\n",
       "  173.10000610351562,\n",
       "  173.18826293945312,\n",
       "  173.147216796875,\n",
       "  173.47874450683594,\n",
       "  173.4799041748047,\n",
       "  173.41903686523438,\n",
       "  173.4008331298828,\n",
       "  173.449951171875,\n",
       "  173.6825714111328,\n",
       "  173.77542114257812,\n",
       "  173.89068603515625,\n",
       "  173.99046325683594,\n",
       "  174.1044464111328,\n",
       "  174.05072021484375,\n",
       "  174.10438537597656,\n",
       "  174.1077880859375,\n",
       "  174.08201599121094,\n",
       "  174.16929626464844,\n",
       "  174.299560546875,\n",
       "  174.19288635253906,\n",
       "  174.2890625,\n",
       "  174.22576904296875,\n",
       "  174.21548461914062,\n",
       "  174.25538635253906,\n",
       "  174.1855010986328,\n",
       "  174.25634765625,\n",
       "  174.2926025390625,\n",
       "  174.3294677734375,\n",
       "  174.31759643554688,\n",
       "  174.353759765625,\n",
       "  174.37306213378906,\n",
       "  174.4514923095703,\n",
       "  174.50843811035156,\n",
       "  174.520751953125,\n",
       "  174.5441131591797,\n",
       "  174.55145263671875,\n",
       "  174.5610809326172,\n",
       "  174.5715789794922,\n",
       "  174.59487915039062,\n",
       "  174.62217712402344,\n",
       "  174.6551055908203,\n",
       "  174.6684112548828],\n",
       " 'val_total_loss': [110.85250091552734,\n",
       "  100.64486694335938,\n",
       "  93.68682098388672,\n",
       "  94.1683120727539,\n",
       "  89.9529037475586,\n",
       "  94.84658813476562,\n",
       "  85.23368072509766,\n",
       "  73.57048797607422,\n",
       "  64.59664154052734,\n",
       "  60.75146484375,\n",
       "  61.362815856933594,\n",
       "  60.35335922241211,\n",
       "  60.982582092285156,\n",
       "  58.266231536865234,\n",
       "  58.79758834838867,\n",
       "  58.718345642089844,\n",
       "  63.52770233154297,\n",
       "  67.52116394042969,\n",
       "  118.25767517089844,\n",
       "  115.72555541992188,\n",
       "  113.5090103149414,\n",
       "  114.9024429321289,\n",
       "  112.88880920410156,\n",
       "  116.04286193847656,\n",
       "  119.99561309814453,\n",
       "  124.81896209716797,\n",
       "  124.25955200195312,\n",
       "  142.43807983398438,\n",
       "  144.71922302246094,\n",
       "  206.55897521972656,\n",
       "  160.61776733398438,\n",
       "  160.0877227783203,\n",
       "  157.54360961914062,\n",
       "  158.0282440185547,\n",
       "  157.91873168945312,\n",
       "  169.23068237304688,\n",
       "  168.6219482421875,\n",
       "  177.76858520507812,\n",
       "  182.455078125,\n",
       "  185.6580352783203,\n",
       "  191.3725128173828,\n",
       "  189.28221130371094,\n",
       "  187.9267578125,\n",
       "  188.32594299316406,\n",
       "  186.8203582763672,\n",
       "  190.7680206298828,\n",
       "  189.7405548095703,\n",
       "  188.55813598632812,\n",
       "  191.00437927246094,\n",
       "  186.9730224609375,\n",
       "  186.4496307373047,\n",
       "  188.25588989257812,\n",
       "  185.40000915527344,\n",
       "  187.64865112304688,\n",
       "  184.0082244873047,\n",
       "  186.33782958984375,\n",
       "  188.4540252685547,\n",
       "  186.36553955078125,\n",
       "  187.04371643066406,\n",
       "  186.85443115234375,\n",
       "  184.4871368408203,\n",
       "  187.34292602539062,\n",
       "  185.17355346679688,\n",
       "  186.7900390625,\n",
       "  186.34857177734375,\n",
       "  185.1352081298828,\n",
       "  186.11492919921875,\n",
       "  185.8313446044922,\n",
       "  186.42474365234375,\n",
       "  188.1607666015625,\n",
       "  188.12574768066406,\n",
       "  187.69674682617188,\n",
       "  188.60694885253906,\n",
       "  186.47528076171875,\n",
       "  187.64169311523438,\n",
       "  187.79635620117188,\n",
       "  188.1534423828125,\n",
       "  186.76756286621094,\n",
       "  188.16355895996094,\n",
       "  186.8472137451172,\n",
       "  187.0399932861328,\n",
       "  186.171875,\n",
       "  186.15785217285156,\n",
       "  187.16607666015625,\n",
       "  187.5236053466797,\n",
       "  186.76913452148438,\n",
       "  186.82516479492188,\n",
       "  186.9686737060547,\n",
       "  186.97412109375,\n",
       "  187.73011779785156,\n",
       "  187.80311584472656,\n",
       "  187.20523071289062,\n",
       "  187.16802978515625,\n",
       "  186.99993896484375,\n",
       "  186.7191162109375,\n",
       "  186.54598999023438,\n",
       "  186.58685302734375,\n",
       "  186.76202392578125,\n",
       "  186.7022705078125,\n",
       "  186.70257568359375],\n",
       " 'val_reconstruction_loss': [110.85250091552734,\n",
       "  100.64486694335938,\n",
       "  93.68682098388672,\n",
       "  94.1683120727539,\n",
       "  89.9529037475586,\n",
       "  94.84658813476562,\n",
       "  85.23368072509766,\n",
       "  73.57048797607422,\n",
       "  64.59664154052734,\n",
       "  60.75146484375,\n",
       "  61.362815856933594,\n",
       "  60.35335922241211,\n",
       "  60.982582092285156,\n",
       "  58.266231536865234,\n",
       "  58.79758834838867,\n",
       "  58.718345642089844,\n",
       "  58.85251235961914,\n",
       "  58.14303207397461,\n",
       "  104.41708374023438,\n",
       "  97.13426208496094,\n",
       "  89.5667953491211,\n",
       "  85.80513763427734,\n",
       "  79.78326416015625,\n",
       "  78.60334014892578,\n",
       "  78.1514892578125,\n",
       "  78.2061767578125,\n",
       "  73.06880950927734,\n",
       "  86.31304931640625,\n",
       "  84.01227569580078,\n",
       "  139.20907592773438,\n",
       "  88.41008758544922,\n",
       "  81.9727554321289,\n",
       "  74.43814849853516,\n",
       "  69.55553436279297,\n",
       "  63.75530242919922,\n",
       "  70.12863159179688,\n",
       "  64.80594635009766,\n",
       "  68.98838806152344,\n",
       "  68.76976776123047,\n",
       "  67.11701965332031,\n",
       "  67.80313110351562,\n",
       "  65.77082061767578,\n",
       "  64.38141632080078,\n",
       "  64.82418823242188,\n",
       "  63.289306640625,\n",
       "  67.23995971679688,\n",
       "  66.23188018798828,\n",
       "  65.18408966064453,\n",
       "  67.65898895263672,\n",
       "  64.08003997802734,\n",
       "  63.77916717529297,\n",
       "  66.45625305175781,\n",
       "  63.66327667236328,\n",
       "  66.60176849365234,\n",
       "  63.197505950927734,\n",
       "  65.3492431640625,\n",
       "  67.19514465332031,\n",
       "  65.16967010498047,\n",
       "  65.65909576416016,\n",
       "  65.3541259765625,\n",
       "  63.17501449584961,\n",
       "  65.69629669189453,\n",
       "  63.749271392822266,\n",
       "  65.19862365722656,\n",
       "  64.80753326416016,\n",
       "  63.64292526245117,\n",
       "  64.44987487792969,\n",
       "  64.14016723632812,\n",
       "  64.67688751220703,\n",
       "  66.23688507080078,\n",
       "  66.23206329345703,\n",
       "  65.77447509765625,\n",
       "  66.66889953613281,\n",
       "  64.60491180419922,\n",
       "  65.75076293945312,\n",
       "  65.7750244140625,\n",
       "  66.11328125,\n",
       "  64.80192565917969,\n",
       "  66.09687042236328,\n",
       "  64.86943054199219,\n",
       "  65.02005004882812,\n",
       "  64.22473907470703,\n",
       "  64.16490173339844,\n",
       "  65.13320922851562,\n",
       "  65.4438247680664,\n",
       "  64.75184631347656,\n",
       "  64.78836822509766,\n",
       "  64.91427612304688,\n",
       "  64.8749771118164,\n",
       "  65.57695770263672,\n",
       "  65.63575744628906,\n",
       "  65.02152252197266,\n",
       "  64.97644805908203,\n",
       "  64.81531524658203,\n",
       "  64.51902770996094,\n",
       "  64.34368896484375,\n",
       "  64.36910247802734,\n",
       "  64.49852752685547,\n",
       "  64.44316864013672,\n",
       "  64.4284439086914],\n",
       " 'val_kl_loss': [175.3036346435547,\n",
       "  169.35299682617188,\n",
       "  171.3735809326172,\n",
       "  172.7078399658203,\n",
       "  172.15457153320312,\n",
       "  172.4905242919922,\n",
       "  172.51210021972656,\n",
       "  173.2223358154297,\n",
       "  174.501220703125,\n",
       "  175.17820739746094,\n",
       "  168.00755310058594,\n",
       "  168.2044677734375,\n",
       "  167.3410186767578,\n",
       "  168.0111083984375,\n",
       "  166.30679321289062,\n",
       "  166.02281188964844,\n",
       "  166.9710235595703,\n",
       "  167.46673583984375,\n",
       "  164.7688751220703,\n",
       "  165.99366760253906,\n",
       "  171.01577758789062,\n",
       "  173.19825744628906,\n",
       "  168.90589904785156,\n",
       "  167.14073181152344,\n",
       "  166.04811096191406,\n",
       "  166.47427368164062,\n",
       "  166.2036895751953,\n",
       "  167.0387420654297,\n",
       "  166.77734375,\n",
       "  171.8109588623047,\n",
       "  171.92308044433594,\n",
       "  174.36376953125,\n",
       "  174.59132385253906,\n",
       "  175.5410919189453,\n",
       "  176.9989471435547,\n",
       "  176.9679412841797,\n",
       "  176.5578155517578,\n",
       "  176.5912628173828,\n",
       "  176.5299835205078,\n",
       "  176.40032958984375,\n",
       "  176.5277099609375,\n",
       "  176.44483947753906,\n",
       "  176.49334716796875,\n",
       "  176.4310760498047,\n",
       "  176.4729461669922,\n",
       "  176.4686737060547,\n",
       "  176.44097900390625,\n",
       "  176.24864196777344,\n",
       "  176.20770263671875,\n",
       "  175.56141662597656,\n",
       "  175.24351501464844,\n",
       "  173.99948120117188,\n",
       "  173.90960693359375,\n",
       "  172.9241180419922,\n",
       "  172.5867156982422,\n",
       "  172.84085083007812,\n",
       "  173.22695922851562,\n",
       "  173.13694763183594,\n",
       "  173.40660095214844,\n",
       "  173.57186889648438,\n",
       "  173.3030548095703,\n",
       "  173.7808837890625,\n",
       "  173.4632568359375,\n",
       "  173.70205688476562,\n",
       "  173.63003540039062,\n",
       "  173.5604248046875,\n",
       "  173.80723571777344,\n",
       "  173.8445281982422,\n",
       "  173.92550659179688,\n",
       "  174.17698669433594,\n",
       "  174.13385009765625,\n",
       "  174.17466735839844,\n",
       "  174.19723510742188,\n",
       "  174.1005401611328,\n",
       "  174.1298828125,\n",
       "  174.31617736816406,\n",
       "  174.3430938720703,\n",
       "  174.23663330078125,\n",
       "  174.3809814453125,\n",
       "  174.25399780273438,\n",
       "  174.31422424316406,\n",
       "  174.21018981933594,\n",
       "  174.27566528320312,\n",
       "  174.3326873779297,\n",
       "  174.39967346191406,\n",
       "  174.31040954589844,\n",
       "  174.33827209472656,\n",
       "  174.36343383789062,\n",
       "  174.42733764648438,\n",
       "  174.50453186035156,\n",
       "  174.52479553222656,\n",
       "  174.54815673828125,\n",
       "  174.55941772460938,\n",
       "  174.54946899414062,\n",
       "  174.5715789794922,\n",
       "  174.57473754882812,\n",
       "  174.59678649902344,\n",
       "  174.66217041015625,\n",
       "  174.65586853027344,\n",
       "  174.67733764648438],\n",
       " 'beta': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.027999999999999997,\n",
       "  0.055999999999999994,\n",
       "  0.08399999999999999,\n",
       "  0.11199999999999999,\n",
       "  0.13999999999999999,\n",
       "  0.16799999999999998,\n",
       "  0.196,\n",
       "  0.22399999999999998,\n",
       "  0.252,\n",
       "  0.27999999999999997,\n",
       "  0.308,\n",
       "  0.33599999999999997,\n",
       "  0.364,\n",
       "  0.392,\n",
       "  0.42,\n",
       "  0.44799999999999995,\n",
       "  0.476,\n",
       "  0.504,\n",
       "  0.5319999999999999,\n",
       "  0.5599999999999999,\n",
       "  0.588,\n",
       "  0.616,\n",
       "  0.644,\n",
       "  0.6719999999999999,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7,\n",
       "  0.7],\n",
       " 'learning_rate': [0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.005,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.002,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taef.train_VAE(\n",
    "    datasets=datasets,\n",
    "    h_dim_1=H_DIM_1,\n",
    "    h_dim_2=H_DIM_2,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    model_path=MODEL_PATH,\n",
    "    epochs=100,\n",
    "    beta=BETA,\n",
    "    annealing_type=ANNEALING_TYPE,\n",
    "    warmup_epochs=WARMUP_EPOCHS,\n",
    "    increase_epochs=INCREASE_EPOCHS,\n",
    "    learning_rates=LEARNING_RATES,\n",
    "    stage_lengths=STAGE_LENGTHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28e43304-48f4-4653-8ead-0d179c8089d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning evaluation procedure... booting up...\n",
      "Calculating AD scores for each dataset...\n",
      "Evaluating hChToTauNu set...\n",
      "23759/23759 [==============================] - 20s 830us/step\n",
      "Evaluating hToTauTau set...\n",
      "21603/21603 [==============================] - 19s 864us/step\n",
      "Evaluating Ato4l set...\n",
      "1750/1750 [==============================] - 2s 894us/step\n",
      "Evaluating leptoquark set...\n",
      "10642/10642 [==============================] - 9s 881us/step\n",
      "Evaluating train set...\n",
      "62500/62500 [==============================] - 58s 920us/step\n",
      "Evaluating val set...\n",
      "31250/31250 [==============================] - 29s 919us/step\n",
      "Evaluating test set...\n",
      "31250/31250 [==============================] - 28s 908us/step\n",
      "Training student networks...\n",
      "Initializing knowledge distillation procedure...\n",
      "Creating neural network student...\n",
      "Starting training of the neural network student.\n",
      "Epoch 1/100\n",
      "15625/15625 [==============================] - 36s 2ms/step - loss: 447.5761 - val_loss: 355.9383 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 452.1303 - val_loss: 348.3939 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 452.4220 - val_loss: 459.1924 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 451.4408 - val_loss: 320.4290 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "15625/15625 [==============================] - 34s 2ms/step - loss: 450.0468 - val_loss: 321.1085 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "15625/15625 [==============================] - 34s 2ms/step - loss: 449.9924 - val_loss: 313.7740 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 450.3764 - val_loss: 315.3348 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 450.9249 - val_loss: 310.6454 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 451.0058 - val_loss: 314.3737 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 448.7298 - val_loss: 311.7009 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 448.9007 - val_loss: 315.3604 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 450.4159 - val_loss: 301.3447 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 450.8775 - val_loss: 307.6726 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 449.3794 - val_loss: 314.4228 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 448.3304 - val_loss: 305.9941 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 448.8792 - val_loss: 311.9460 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 445.0036 - val_loss: 308.0268 - lr: 1.0000e-03\n",
      "Epoch 18/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 446.1955 - val_loss: 300.5989 - lr: 1.0000e-03\n",
      "Epoch 19/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 445.1964 - val_loss: 303.5386 - lr: 1.0000e-03\n",
      "Epoch 20/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 444.6074 - val_loss: 302.4275 - lr: 1.0000e-03\n",
      "Epoch 21/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 444.2827 - val_loss: 303.6573 - lr: 1.0000e-03\n",
      "Epoch 22/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 444.4158 - val_loss: 304.6951 - lr: 1.0000e-03\n",
      "Epoch 23/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 443.7083 - val_loss: 301.2646 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 444.5238 - val_loss: 301.0839 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "15625/15625 [==============================] - 34s 2ms/step - loss: 445.0040 - val_loss: 301.4458 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "15625/15625 [==============================] - 35s 2ms/step - loss: 442.6823 - val_loss: 302.9071 - lr: 1.0000e-04\n",
      "Saving student networks...\n",
      "Student networks saved! Knowledge distillation complete.\n",
      "Running inference on the student networks...\n",
      "23759/23759 [==============================] - 16s 659us/step\n",
      "21603/21603 [==============================] - 14s 668us/step\n",
      "1750/1750 [==============================] - 1s 685us/step\n",
      "10642/10642 [==============================] - 7s 674us/step\n",
      "62500/62500 [==============================] - 44s 703us/step\n",
      "31250/31250 [==============================] - 21s 685us/step\n",
      "31250/31250 [==============================] - 21s 684us/step\n",
      "Inference complete! Plotting student performance...\n",
      "Plotting ROC curves...\n",
      "ROC curves plotted! Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "datasets = taef.evaluate_VAE(\n",
    "    datasets=datasets,\n",
    "    model_path=MODEL_PATH,\n",
    "    plots_path=PLOTS_PATH,\n",
    "    h_dim_1=H_DIM_1,\n",
    "    h_dim_2=H_DIM_2,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    beta=BETA,\n",
    "    train_students=True,\n",
    "    load_students=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2164d92d-dab6-49dc-803b-0b55ee7aa2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82.17086029  0.         -1.56314683  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.         26.40037155  0.99070734  2.22225761\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "[ 8.32446060e+01  4.77232039e-03 -1.64192408e-01  2.43176460e-01\n",
      " -6.67954385e-02  1.35651305e-01  1.82808340e-02  2.49776356e-02\n",
      " -2.34766603e-02 -3.58777028e-03  5.79275191e-04 -3.52889299e-04\n",
      " -7.38658011e-04 -3.21843475e-03 -2.79551372e-04  2.64014816e+01\n",
      " -1.76995188e-01  9.39366519e-02 -7.22818077e-03  1.81436501e-02\n",
      "  5.40688634e-03 -8.44836235e-04 -3.23005766e-03  2.44379044e-05\n",
      " -6.70820475e-04 -3.56927514e-04  4.74974513e-05  2.44202137e-01\n",
      " -1.67458132e-01 -1.58292949e-01  3.58527660e-01 -1.03340700e-01\n",
      " -4.71231341e-03 -4.73961830e-02 -3.79919112e-02  6.81253299e-02\n",
      " -1.42818362e-01 -7.74823129e-03  2.95025259e-02 -1.17517576e-01\n",
      "  5.94439283e-02  9.52288508e-03  1.26258638e-02 -4.41521406e-03\n",
      " -1.13751292e-02 -4.29497808e-02 -1.65509433e-03 -2.61606276e-03\n",
      "  7.86595419e-03 -1.86863914e-03  5.11728227e-04 -5.61723020e-04\n",
      " -1.21712685e-03 -5.74214756e-03  6.40110113e-04 -4.41576540e-03\n",
      "  2.32699513e-03]\n",
      "[-168.40921   -24.950583  309.23114 ]\n",
      "[-461.90875 -171.04306 -104.36362]\n"
     ]
    }
   ],
   "source": [
    "tag = 'test'\n",
    "idx=0\n",
    "print(datasets[tag]['data'][idx])\n",
    "print(datasets[tag]['y_pred'][idx])\n",
    "print(datasets[tag]['z_mean'][idx])\n",
    "print(datasets[tag]['z_log_var'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6573b0-921b-487a-b0ca-324868264ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.12.0",
   "language": "python",
   "name": "tensorflow-2.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
